{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e452db2c-ea9f-4d26-af16-806eb4fbe392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7380fc3a-572a-44bc-b1d8-a9706d121b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Target</th>\n",
       "      <th>RAV</th>\n",
       "      <th>Volitility</th>\n",
       "      <th>Buy_Sell_Strength</th>\n",
       "      <th>Weighted_Strength</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Log_returns</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-02-06</th>\n",
       "      <td>26.495506</td>\n",
       "      <td>174826400</td>\n",
       "      <td>1</td>\n",
       "      <td>2.508224e+08</td>\n",
       "      <td>1.046604</td>\n",
       "      <td>0.266668</td>\n",
       "      <td>-0.162635</td>\n",
       "      <td>-0.007055</td>\n",
       "      <td>-0.008421</td>\n",
       "      <td>8.064544</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-09</th>\n",
       "      <td>26.671505</td>\n",
       "      <td>155559200</td>\n",
       "      <td>1</td>\n",
       "      <td>2.247958e+08</td>\n",
       "      <td>1.096023</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>0.287110</td>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.006643</td>\n",
       "      <td>8.139062</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-10</th>\n",
       "      <td>27.183905</td>\n",
       "      <td>248034000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.123746e+08</td>\n",
       "      <td>1.149111</td>\n",
       "      <td>0.934671</td>\n",
       "      <td>0.507655</td>\n",
       "      <td>0.037193</td>\n",
       "      <td>0.019212</td>\n",
       "      <td>8.278700</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-11</th>\n",
       "      <td>27.821060</td>\n",
       "      <td>294247200</td>\n",
       "      <td>1</td>\n",
       "      <td>2.185590e+08</td>\n",
       "      <td>1.245724</td>\n",
       "      <td>0.983471</td>\n",
       "      <td>0.650899</td>\n",
       "      <td>0.053246</td>\n",
       "      <td>0.023439</td>\n",
       "      <td>8.423923</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-12</th>\n",
       "      <td>28.173054</td>\n",
       "      <td>297898000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.314498e+08</td>\n",
       "      <td>1.339346</td>\n",
       "      <td>0.465967</td>\n",
       "      <td>-0.043803</td>\n",
       "      <td>0.043885</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>8.471078</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Close     Volume  Target           RAV  Volitility  \\\n",
       "Date                                                                 \n",
       "2015-02-06  26.495506  174826400       1  2.508224e+08    1.046604   \n",
       "2015-02-09  26.671505  155559200       1  2.247958e+08    1.096023   \n",
       "2015-02-10  27.183905  248034000       1  2.123746e+08    1.149111   \n",
       "2015-02-11  27.821060  294247200       1  2.185590e+08    1.245724   \n",
       "2015-02-12  28.173054  297898000       1  2.314498e+08    1.339346   \n",
       "\n",
       "            Buy_Sell_Strength  Weighted_Strength     Trend   Returns  \\\n",
       "Date                                                                   \n",
       "2015-02-06           0.266668          -0.162635 -0.007055 -0.008421   \n",
       "2015-02-09           0.914897           0.287110  0.010177  0.006643   \n",
       "2015-02-10           0.934671           0.507655  0.037193  0.019212   \n",
       "2015-02-11           0.983471           0.650899  0.053246  0.023439   \n",
       "2015-02-12           0.465967          -0.043803  0.043885  0.012652   \n",
       "\n",
       "            Log_returns  AAPL   MSFT   NVDA   TSLA  \n",
       "Date                                                \n",
       "2015-02-06     8.064544  True  False  False  False  \n",
       "2015-02-09     8.139062  True  False  False  False  \n",
       "2015-02-10     8.278700  True  False  False  False  \n",
       "2015-02-11     8.423923  True  False  False  False  \n",
       "2015-02-12     8.471078  True  False  False  False  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/engineered_features_multi_stock_data.csv\",index_col=\"Date\")\n",
    "df.drop(\"Unnamed: 0\",axis=\"columns\",inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fce56f67-4e9a-4562-bc7e-7f20f51fa94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalu\\AppData\\Local\\Temp\\ipykernel_22036\\3098634208.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 0.95380477  0.66382358  2.0556165  ... -1.1006561  -0.74756511\n",
      " -1.40813909]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_std.loc[mask, num_features] = scaler.fit_transform(X.loc[mask, num_features])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(\"Target\", axis=\"columns\")\n",
    "X_std = X.copy()\n",
    "scaler = StandardScaler()\n",
    "num_features = ['Close', 'Volume', 'RAV', 'Volitility', 'Buy_Sell_Strength', 'Weighted_Strength', 'Trend']\n",
    "tickers = ['AAPL', 'MSFT', 'NVDA', 'TSLA']\n",
    "for ticker in tickers:\n",
    "    mask = X[ticker] == 1\n",
    "    X_std.loc[mask, num_features] = scaler.fit_transform(X.loc[mask, num_features])\n",
    "\n",
    "\n",
    "y = df.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bdf7eb61-860b-4ea8-a7c9-cb4f8f6d56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_split(x,y,model):\n",
    "#     tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "#     for train_idx, test_idx in tscv.split(X):\n",
    "#         X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#         y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "#         model.fit(X_train, y_train)\n",
    "#         preds = model.predict(X_test)\n",
    "    \n",
    "#     print(classification_report(y_test,preds))\n",
    "    \n",
    "#     print(f'Train: {model.score(X_train,y_train)}')\n",
    "#     print(f'Test: {model.score(X_test,y_test)}')\n",
    "    \n",
    "#     scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')\n",
    "#     print(\"Scores:\", scores)\n",
    "#     print(\"Mean:\", scores.mean())\n",
    "\n",
    "#     return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0cc5c8ac-698c-4ead-a405-de1f860e07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(x,y,model):\n",
    "    test_ratio = 0.2\n",
    "    train_list, test_list = [], []\n",
    "    \n",
    "    for ticker, group in df.groupby(df[['AAPL','MSFT','NVDA','TSLA']].idxmax(axis=1).map(lambda x: x if pd.notna(x) else None)):\n",
    "        group = group.sort_values('Date')  # ensure sorted by time\n",
    "        split_idx = int(len(group) * (1 - test_ratio))\n",
    "        train_list.append(group.iloc[:split_idx])\n",
    "        test_list.append(group.iloc[split_idx:])\n",
    "\n",
    "    train_df = pd.concat(train_list).sort_values('Date').reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    X_train = train_df.drop(columns=['Target'])\n",
    "    y_train = train_df['Target']\n",
    "    \n",
    "    X_test = test_df.drop(columns=['Target'])\n",
    "    y_test = test_df['Target']\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test,preds))\n",
    "    \n",
    "    print(f'Train: {model.score(X_train,y_train)}')\n",
    "    print(f'Test: {model.score(X_test,y_test)}')\n",
    "    \n",
    "    scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a29ce432-6476-49e3-8d7f-673a4ae2764f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.39      0.41       988\n",
      "           1       0.53      0.58      0.55      1167\n",
      "\n",
      "    accuracy                           0.49      2155\n",
      "   macro avg       0.48      0.48      0.48      2155\n",
      "weighted avg       0.49      0.49      0.49      2155\n",
      "\n",
      "Train: 0.5963057620817844\n",
      "Test: 0.49280742459396754\n",
      "Scores: [0.51199108 0.50139431 0.54043503 0.47462354 0.52649191]\n",
      "Mean: 0.5109871723368655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           Close     Volume           RAV  Volitility  Buy_Sell_Strength  \\\n",
       " 0      46.935463  692573000  5.937243e+08    1.645283           0.965530   \n",
       " 1     231.279999  135702700  1.130020e+08   15.801932           0.926406   \n",
       " 2     317.101318   24040000  2.155843e+07    8.276232           0.844679   \n",
       " 3     233.190002  130442800  1.174883e+08   15.491407           0.322981   \n",
       " 4      45.637337  755293000  6.254683e+08    1.644891           0.117379   \n",
       " ...          ...        ...           ...         ...                ...   \n",
       " 2150  510.959991   24065400  1.830163e+07    6.116654           0.095339   \n",
       " 2151  249.100006   17891929  4.003839e+07    6.819915           0.900764   \n",
       " 2152  513.518494    6387641  1.618231e+07    6.132103           0.388691   \n",
       " 2153  188.149994   89180834  1.577492e+08    5.938565           0.526582   \n",
       " 2154  426.689209   38039420  8.715610e+07   11.718095           0.752337   \n",
       " \n",
       "       Weighted_Strength     Trend   Returns  Log_returns   AAPL   MSFT   NVDA  \\\n",
       " 0              0.543035 -0.048846  0.084713    12.458125  False  False   True   \n",
       " 1              0.512066 -0.056184  0.073275    43.045538  False  False  False   \n",
       " 2              0.384355 -0.057114  0.017063    55.221980  False   True  False   \n",
       " 3             -0.196537 -0.074962  0.008258    42.837232  False  False  False   \n",
       " 4             -0.462040 -0.076714 -0.027658    11.857631  False  False   True   \n",
       " ...                 ...       ...       ...          ...    ...    ...    ...   \n",
       " 2150          -0.532101  0.038578 -0.021899    81.643431  False   True  False   \n",
       " 2151           0.179089  0.033857  0.015615    45.271487   True  False  False   \n",
       " 2152          -0.043937  0.033574  0.005007    82.343571  False   True  False   \n",
       " 2153           0.015028  0.026759  0.027244    36.110747  False  False   True   \n",
       " 2154           0.110133  0.061585  0.031921    70.824096  False  False  False   \n",
       " \n",
       "        TSLA  \n",
       " 0     False  \n",
       " 1      True  \n",
       " 2     False  \n",
       " 3      True  \n",
       " 4     False  \n",
       " ...     ...  \n",
       " 2150  False  \n",
       " 2151  False  \n",
       " 2152  False  \n",
       " 2153  False  \n",
       " 2154   True  \n",
       " \n",
       " [2155 rows x 13 columns],\n",
       " 0       0\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2150    1\n",
       " 2151    0\n",
       " 2152    0\n",
       " 2153    0\n",
       " 2154    0\n",
       " Name: Target, Length: 2155, dtype: int64)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,     \n",
    "    learning_rate=0.08,    \n",
    "    max_depth=3,         \n",
    "    subsample=1.0,        \n",
    "    colsample_bytree=0.8, \n",
    "    gamma=0,              \n",
    "    reg_alpha=1,          \n",
    "    reg_lambda=2,         \n",
    "    random_state=42\n",
    ")\n",
    "data_split(X,y,xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "cd580330-4779-4e98-9fbc-a41137c117c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.39      0.41       988\n",
      "           1       0.53      0.58      0.55      1167\n",
      "\n",
      "    accuracy                           0.49      2155\n",
      "   macro avg       0.48      0.48      0.48      2155\n",
      "weighted avg       0.49      0.49      0.49      2155\n",
      "\n",
      "Train: 0.5963057620817844\n",
      "Test: 0.49280742459396754\n",
      "Scores: [0.51199108 0.50139431 0.54043503 0.47462354 0.52649191]\n",
      "Mean: 0.5109871723368655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           Close     Volume           RAV  Volitility  Buy_Sell_Strength  \\\n",
       " 0      46.935463  692573000  5.937243e+08    1.645283           0.965530   \n",
       " 1     231.279999  135702700  1.130020e+08   15.801932           0.926406   \n",
       " 2     317.101318   24040000  2.155843e+07    8.276232           0.844679   \n",
       " 3     233.190002  130442800  1.174883e+08   15.491407           0.322981   \n",
       " 4      45.637337  755293000  6.254683e+08    1.644891           0.117379   \n",
       " ...          ...        ...           ...         ...                ...   \n",
       " 2150  510.959991   24065400  1.830163e+07    6.116654           0.095339   \n",
       " 2151  249.100006   17891929  4.003839e+07    6.819915           0.900764   \n",
       " 2152  513.518494    6387641  1.618231e+07    6.132103           0.388691   \n",
       " 2153  188.149994   89180834  1.577492e+08    5.938565           0.526582   \n",
       " 2154  426.689209   38039420  8.715610e+07   11.718095           0.752337   \n",
       " \n",
       "       Weighted_Strength     Trend   Returns  Log_returns   AAPL   MSFT   NVDA  \\\n",
       " 0              0.543035 -0.048846  0.084713    12.458125  False  False   True   \n",
       " 1              0.512066 -0.056184  0.073275    43.045538  False  False  False   \n",
       " 2              0.384355 -0.057114  0.017063    55.221980  False   True  False   \n",
       " 3             -0.196537 -0.074962  0.008258    42.837232  False  False  False   \n",
       " 4             -0.462040 -0.076714 -0.027658    11.857631  False  False   True   \n",
       " ...                 ...       ...       ...          ...    ...    ...    ...   \n",
       " 2150          -0.532101  0.038578 -0.021899    81.643431  False   True  False   \n",
       " 2151           0.179089  0.033857  0.015615    45.271487   True  False  False   \n",
       " 2152          -0.043937  0.033574  0.005007    82.343571  False   True  False   \n",
       " 2153           0.015028  0.026759  0.027244    36.110747  False  False   True   \n",
       " 2154           0.110133  0.061585  0.031921    70.824096  False  False  False   \n",
       " \n",
       "        TSLA  \n",
       " 0     False  \n",
       " 1      True  \n",
       " 2     False  \n",
       " 3      True  \n",
       " 4     False  \n",
       " ...     ...  \n",
       " 2150  False  \n",
       " 2151  False  \n",
       " 2152  False  \n",
       " 2153  False  \n",
       " 2154   True  \n",
       " \n",
       " [2155 rows x 13 columns],\n",
       " 0       0\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2150    1\n",
       " 2151    0\n",
       " 2152    0\n",
       " 2153    0\n",
       " 2154    0\n",
       " Name: Target, Length: 2155, dtype: int64)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split(X_std,y,xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a226f1d1-c9cb-4424-a06b-e10c99a04c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.76      0.58       988\n",
      "           1       0.58      0.28      0.37      1167\n",
      "\n",
      "    accuracy                           0.50      2155\n",
      "   macro avg       0.52      0.52      0.48      2155\n",
      "weighted avg       0.53      0.50      0.47      2155\n",
      "\n",
      "Train: 0.547746282527881\n",
      "Test: 0.4979118329466357\n",
      "Scores: [0.49804796 0.50864473 0.5376464  0.48689347 0.51645287]\n",
      "Mean: 0.5095370886781929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           Close     Volume           RAV  Volitility  Buy_Sell_Strength  \\\n",
       " 0      46.935463  692573000  5.937243e+08    1.645283           0.965530   \n",
       " 1     231.279999  135702700  1.130020e+08   15.801932           0.926406   \n",
       " 2     317.101318   24040000  2.155843e+07    8.276232           0.844679   \n",
       " 3     233.190002  130442800  1.174883e+08   15.491407           0.322981   \n",
       " 4      45.637337  755293000  6.254683e+08    1.644891           0.117379   \n",
       " ...          ...        ...           ...         ...                ...   \n",
       " 2150  510.959991   24065400  1.830163e+07    6.116654           0.095339   \n",
       " 2151  249.100006   17891929  4.003839e+07    6.819915           0.900764   \n",
       " 2152  513.518494    6387641  1.618231e+07    6.132103           0.388691   \n",
       " 2153  188.149994   89180834  1.577492e+08    5.938565           0.526582   \n",
       " 2154  426.689209   38039420  8.715610e+07   11.718095           0.752337   \n",
       " \n",
       "       Weighted_Strength     Trend   Returns  Log_returns   AAPL   MSFT   NVDA  \\\n",
       " 0              0.543035 -0.048846  0.084713    12.458125  False  False   True   \n",
       " 1              0.512066 -0.056184  0.073275    43.045538  False  False  False   \n",
       " 2              0.384355 -0.057114  0.017063    55.221980  False   True  False   \n",
       " 3             -0.196537 -0.074962  0.008258    42.837232  False  False  False   \n",
       " 4             -0.462040 -0.076714 -0.027658    11.857631  False  False   True   \n",
       " ...                 ...       ...       ...          ...    ...    ...    ...   \n",
       " 2150          -0.532101  0.038578 -0.021899    81.643431  False   True  False   \n",
       " 2151           0.179089  0.033857  0.015615    45.271487   True  False  False   \n",
       " 2152          -0.043937  0.033574  0.005007    82.343571  False   True  False   \n",
       " 2153           0.015028  0.026759  0.027244    36.110747  False  False   True   \n",
       " 2154           0.110133  0.061585  0.031921    70.824096  False  False  False   \n",
       " \n",
       "        TSLA  \n",
       " 0     False  \n",
       " 1      True  \n",
       " 2     False  \n",
       " 3      True  \n",
       " 4     False  \n",
       " ...     ...  \n",
       " 2150  False  \n",
       " 2151  False  \n",
       " 2152  False  \n",
       " 2153  False  \n",
       " 2154   True  \n",
       " \n",
       " [2155 rows x 13 columns],\n",
       " 0       0\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2150    1\n",
       " 2151    0\n",
       " 2152    0\n",
       " 2153    0\n",
       " 2154    0\n",
       " Name: Target, Length: 2155, dtype: int64)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, class_weight='balanced', max_depth=3, random_state=42, min_samples_split=5, bootstrap=False)\n",
    "\n",
    "data_split(X,y,rf_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "78a12fc7-216c-4f06-b049-2e93ec47ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.76      0.58       988\n",
      "           1       0.58      0.28      0.38      1167\n",
      "\n",
      "    accuracy                           0.50      2155\n",
      "   macro avg       0.52      0.52      0.48      2155\n",
      "weighted avg       0.53      0.50      0.47      2155\n",
      "\n",
      "Train: 0.546003717472119\n",
      "Test: 0.49837587006960554\n",
      "Scores: [0.49804796 0.50697156 0.53597323 0.48577803 0.51645287]\n",
      "Mean: 0.5086447295036252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_std_classifier = RandomForestClassifier(n_estimators=200, class_weight='balanced', max_depth=3, random_state=42, min_samples_split=3, bootstrap=False)\n",
    "\n",
    "rf_X_test, rf_y_test = data_split(X_std,y,rf_std_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6c4d3013-bcca-4de6-b93f-86462936fc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.406, Best F1: 0.703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.71      0.57       988\n",
      "           1       0.58      0.34      0.43      1167\n",
      "\n",
      "    accuracy                           0.51      2155\n",
      "   macro avg       0.53      0.52      0.50      2155\n",
      "weighted avg       0.53      0.51      0.49      2155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Get prediction probabilities from your model\n",
    "y_probs = rf_std_classifier.predict_proba(rf_X_test)[:, 1]  # probabilities for class 1\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(rf_y_test, y_probs)\n",
    "\n",
    "# Compute F1 for each threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "\n",
    "# Find best threshold for highest F1\n",
    "best_idx = f1_scores.argmax()\n",
    "best_thresh = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"Best Threshold: {best_thresh:.3f}, Best F1: {best_f1:.3f}\")\n",
    "\n",
    "# Evaluate using that threshold\n",
    "y_pred_opt = (y_probs >= 0.493).astype(int)\n",
    "print(classification_report(rf_y_test, y_pred_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c9777807-4fc9-4a26-8ce8-9665a7975c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49312409724597134"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(rf_y_test, y_probs)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3fa1a541-55da-4813-b7ee-d6fdc790a8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGwCAYAAAB7MGXBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfI0lEQVR4nO3dd3xT5f4H8M/J7G4ppYsOyi4gW6YoCBYQUfQqKFxxAF5ciIheuHgFceBERAFR1r1cRRzADwUZyl6yWmZlFlpoSynQpjtNcn5/nCZtaFqakvZkfN6vV17P6clJ8s0hNN8+z/d5jiCKoggiIiIimSjkDoCIiIg8G5MRIiIikhWTESIiIpIVkxEiIiKSFZMRIiIikhWTESIiIpIVkxEiIiKSlUruAGrCZDIhPT0d/v7+EARB7nCIiIioBkRRRF5eHiIjI6FQVN3/4RLJSHp6OqKjo+UOg4iIiGohLS0NUVFRVd7vEsmIv78/AOnNBAQEyBwNERER1YROp0N0dLTle7wqLpGMmIdmAgICmIwQERG5mFuVWLCAlYiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGRldzKyY8cODB06FJGRkRAEAWvWrLnlY7Zv344uXbrAy8sLTZs2xVdffVWbWImIiMgN2Z2MFBQUoEOHDvjyyy9rdHxKSgruv/9+9OnTB4mJifjXv/6FCRMm4Oeff7Y7WCIiInI/dl8ob/DgwRg8eHCNj//qq68QExODOXPmAADi4+Nx8OBBfPLJJ/jb3/5m78s7VkE2UFpovU+pkW4qL0ClBRRKeWIjIiKqBxm5RTCJQENfDbzU8nzn1flVe/fu3YuEhASrfQMHDsTixYtRWloKtVpd6TElJSUoKSmx/KzT6eomuN/+CRz/qfpjFOryxMR8y78K9HoZ8AoANH6A1k9qlRrgFlcmrHOCEmgQCwQ0lj8WIiJyem/8dBQ7z2Tj08c64G9domSJoc6TkczMTISFhVntCwsLg8FgQHZ2NiIiIio9ZtasWXj77bfrOjRAWZZomIkiYNQDEMv3mUoBfSmgz7N+7Lb36z6+2+HTEOg4CmjYHND4AtoAKXnSBgDeQYB/BJMVIiJCQYkBAOCrrfOUoEr18srCTV96oija3G82depUTJo0yfKzTqdDdHS04wN7+CvpZh0cYDIAhpKyW3HZrQQwlgApOwFdOlBaAJTkA/oCQJ8PlOQBxlLHx2gvYwlw/TxQeA3YM7fq47yDgejuQEx3qY3sBKi96y9OIiJyCgUlRgCAnzsnI+Hh4cjMzLTal5WVBZVKhYYNG9p8jFarhVarrevQbBMEqcdEqZaGX24W2an+Y7KXLh3Y8QmQlyElV/p8oEQHFOvK2lyg6Dpw+jfpBkhDTN3HA32nSD0pRETkEfItPSPy1UjWeTLSs2dP/PLLL1b7Nm3ahK5du9qsFyEHCIgEHphd9f0GPZB5DEjbB6SW3QqypJ6UPXOB2LuAB+cCDZvVX8xERCSLAr2UjLhUz0h+fj7Onj1r+TklJQVJSUkIDg5GTEwMpk6disuXL+O///0vAGD8+PH48ssvMWnSJIwbNw579+7F4sWLsWLFCse9C7KPSgNEdZFuPV+Uek/WPA8cKfs3ubgL+KIz0CAO6P4PIO4ewD+8vMZEGwgouF4eEZE7cMmakYMHD6Jfv36Wn821HU899RSWLVuGjIwMpKamWu6Pi4vD+vXr8eqrr2LevHmIjIzE3Llz5Z/WS+UEobx+Ju0A8PMYIOcicCMF2DCl8vFqXyCsrZSoBDcFGrXi0A4RkQsqMRhRapTqOOVMRgTRXE3qxHQ6HQIDA5Gbm4uAgAC5w3F/RoOUiJzbKk19zkqWak2q4hcGjNkENGhSbyESEdHtu16gR+d3NgMAzr43GCqlY3u9a/r9LV8aRM5LqQJCWki37s+VzTCSqq0hGoEbF4GjK4HjP0tJS/4VYPXzwLO/yRs3ERHZxTxEo1UpHJ6I2IMD/3RrgiAlKEqVtOhbo5ZA/38DryQBL/wpHZO6B1h4j6xhEhGRfZyheBVgMkK3K7Q1ENJS2s5IAs5vkzMaIiKygzMUrwJMRsgRxv5Rvr3ySUBfWPWxRETkNPLLFjxjMkKuzyugPCEp0QFLBkprmRARkVMz94z4ybjgGcBkhBylcRfgzrHSduZRIHG5vPEQEdEt5XOYhtyKIABDPgX6TZN+XjcJOLVB3piIiKharBkh99R7IhBQdgnqFSOA3EuyhkNERFUr1JfVjGg4TEPuRKUBnttW/vPZP6o8lIiI5MVhGnJffo2APq9J22n75Y2FiIiqVF7AymSE3FFUN6m9sEPeOIiIqErsGSH3FtZGanNSgewz8sZCREQ2sYCV3Ju5iBUAUrbLFwcREVWpoGzRM64zQu5JoSivG7mcKG8sRERkk2WYRsOeEXJX5rqRpP9JV/4lIiKnwmEacn/R3cq309k7QkTkbCzrjDAZIbflEwxoA6TtHR/LGwsREVWSz2vTkEfoNUFq8zLkjYOIiKyIoshhGvIQrQZLbXoiYDLJGwsREVmUGEwwmKR6PiYj5N4aNivfzkiSLQwiIrJm7hUBOJuG3J3aG4jpJW1/04+zaoiInIR5jRFvtRJKhSBrLExGqO49OLd8+8Qq+eIgIiILZ1kKHmAyQvUhpAUQWrY8/Krn5I2FiIgAAIV6czIi70wagMkI1Zfer0ityQBc2C1vLERE5DSrrwJMRqi+3DEc8G0kbV86IG8sRERU4bo0TEbIUygUQLu/SduF2fLGQkREFdYY4TANeZKwdlK75wugtFjeWIiIPBwLWMkztRxYvn1+m2xhEBFRec8Ih2nIs/iFAnH3SNuXD8obCxGRh8vXs2eEPFWbB6U26Tt54yAi8nCFZQWsvhrWjJCnibpTanWXgfPb5Y2FiMiDOctF8gAmI1TfwtsDCrW0/csr8sZCROTBWMBKnksQgIHvSds5qUBOmrzxEBF5qAI9C1jJk905DojoCIhG4Mj3ckdDROSR8s01I0xGyCMpFEDrIdL2tvelHhIiIqpXXPSMqPdEqRVNQPIvsoZCROSJuM4IkUoD3DdT2j68XN5YiIg8EGfTEAFAy0FSezUZyL0kbyxERB5EFEUU6M3rjDAZIU/WqBXgHyFt/7Ve3liIiDxIicEEo0kEwJoRIqDbOKn97XV54yAi8iDmNUYA9owQATE9y7d5JV8ionphrhfx0SihUAgyR8NkhOQW3aN8+9J++eIgIvIgzrT6KsBkhOSmUAAN4qRtXbq8sRAReYiCsgXPnGFaL8BkhJxBbG+pzTgqbxxERB7CvBS8MxSvAkxGyBlEdZXa4z/JGwcRkYcorxlhzwiRpEWC1OZf4VANEVE9cKbVVwEmI+QMAhuXb9+4KF8cREQewpkukgcwGSFnEdVNaguy5I2DiMgDlPeMsGaEqJxvI6k9vVHeOIiIPIDlujSsGSGqIKZsvZFjLGIlIqprXGeEyJZ2j0itsQQ4v13eWIiI3BwLWIlsCWgMKMr+U3CohoioTlmu2MtkhKgCQQCGfi5t75sH5LOQlYiorlhqRljASnST+AcBjZ+0vWiAvLEQEbkxFrASVcUrAHhonrSdcxFI/J+88RARuSkWsBJVp+2w8u3z2+SKgojIrfFCeUS3Mny51J5YA2SfkTUUIiJ35BY1I/Pnz0dcXBy8vLzQpUsX7Ny5s9rjv/32W3To0AE+Pj6IiIjAM888g2vXrtUqYPIArR8AQtsCplLgu+GAKModERGR2xBF0XLVXpftGVm5ciUmTpyIadOmITExEX369MHgwYORmppq8/hdu3Zh9OjRGDNmDE6cOIEff/wRBw4cwNixY287eHJTCgUw5FNp+/p5IHG5vPEQEbmR4lITTGV/47lszcjs2bMxZswYjB07FvHx8ZgzZw6io6OxYMECm8fv27cPTZo0wYQJExAXF4e77roL//jHP3Dw4MEqX6OkpAQ6nc7qRh4mtifQ4wVpe/dceWMhInIj5uJVAPBWu+AwjV6vx6FDh5CQkGC1PyEhAXv27LH5mF69euHSpUtYv349RFHElStX8NNPP2HIkCFVvs6sWbMQGBhouUVHR9sTJrmLNsOk9toZXs2XiMhByqf1KqFQCDJHI7ErGcnOzobRaERYWJjV/rCwMGRmZtp8TK9evfDtt99ixIgR0Gg0CA8PR1BQEL744osqX2fq1KnIzc213NLS0uwJk9xF1J3l2weXyBcHEZEbcbZpvUAtC1gFwTqTEkWx0j6zkydPYsKECXjrrbdw6NAhbNiwASkpKRg/fnyVz6/VahEQEGB1Iw+kUAC9J0rbx34EjKWyhkNE5A6c7bo0gJ3JSEhICJRKZaVekKysrEq9JWazZs1C79698frrr6N9+/YYOHAg5s+fjyVLliAjI6P2kZNn6PcvwDcU0F0GVo0DSvLljoiIyKWZZ9K4bM+IRqNBly5dsHnzZqv9mzdvRq9evWw+prCwEAqF9csolVLBjMgpm3QrKi3Q7Tlp+8RqYMs78sZDROTi8kvMF8lzjuJVoBbDNJMmTcKiRYuwZMkSJCcn49VXX0Vqaqpl2GXq1KkYPXq05fihQ4di1apVWLBgAc6fP4/du3djwoQJ6NatGyIjIx33Tsh99RgPBJYVMZ/ZXP2xRERUrUInHKaxO5IRI0bg2rVrmDlzJjIyMtCuXTusX78esbGxAICMjAyrNUeefvpp5OXl4csvv8Rrr72GoKAg3Hvvvfjwww8d9y7IvWn9gef3AB82Aa6fA66eBhq1lDsqIiKX5IwFrILoAmMlOp0OgYGByM3NZTGrJ/v2MeDMJqDfNOCeN+SOhojIJX3++xl89vtpPNEtBrMeuaNOX6um39+8Ng25jtZla9NsfQ/QF8gbCxGRiypfCt6Fa0aIZNNiYPn2rjmyhUFE5MqccZiGyQi5joAIoHNZcfTeL3kBPSKiWnD5dUaIZNd/utSWFgL5V+SNhYjIBRWwZ4ToNvmGAA2aSNsn18oaChGRKyqwrDPCZISo9rwCpfbAInnjICJyQSxgJXKEftOkNuciYDLJGwsRkYuxFLBq2DNCVHvN+gMKFWAoBnJTb308ERFZsGaEyBGUKiC4mbT9x0x5YyEicjGsGSFylLC2UnvjorxxEBG5EFEUK1y1lzUjRLfnzjFSe+UE1xshIqqhQr3R8iuT64wQ3S7z9F5DEZC6V9ZQiIhchblXRCEA3mr2jBDdnsAoQOUtbZ/9Xd5YiIhchKVeRKOCIAgyR1OOyQi5rhb3SW3hNXnjICJyEc44kwZgMkKurOk9UluQLW8cREQuwrzGiI8TFa8CTEbIlam8pNaolzcOIiIX4YwXyQOYjJAr8w6W2uvn5Y2DiMhFOOPqqwCTEXJlTXpL7bWzQOF1eWMhInIBzrjgGcBkhFyZVyCgLbtoHotYiYhuqXyYhjUjRI7j10hqdZfljYOIyAWUr77KnhEix2nYXGqvnZM3DiIiF8ACVqK6ENJCajOPyRsHEZELyGfNCFEdiL1Las/+AZhM8sZCROTkzD0jPhrWjBA5TtzdgMYfyE0FLh+UOxoiIqfGYRqiuqDxAeL6SNvpifLGQkTk5PK5HDxRHTFfwVeXLmsYRETOzjybhj0jRI7m3UBqi27IGwcRkZMrZAErUR0xJyNc+IyIqFrlwzQsYCVyLN8QqWUyQkRULRawEtUVn4ZSW5AtbxxERE7MZBJRoJeGaXx4oTwiB9P6S62+QN44iIicWGGp0bLNnhEiR9OUJSN5nE1DRFQV8xCNQgC81M719e9c0RDVhk9w+XaxTr44iIicWMU1RgRBkDkaa0xGyPVVTEbyr8gXBxGREzNP63W2IRqAyQi5i+BmUpufJW8cREROyllXXwWYjJC78AuTWvaMEBFZEUURQHnNCJMRoroSGCW1uWnyxkFE5ESOpOWgy7u/44eDaRWWgneuBc8AJiPkLoJipDYnVd44iIicyEsrDuN6gR5v/HTUMkzjbGuMAExGyF2Yk5EbF+WNg4jIiRiNomXbWVdfBZiMkLtgzwgRUSVihe18y0XyOExDVDcaxEpt9ilAFKs/lixEUYSuuFTuMIiojlT8dVjIAlaiOhYQVb59I0W+OFzMm2uOo/PMzUjO4GJxRO5IrNA3YilgZc0IUR1Racq3cy/LF4eL+fbPVBhMIuZvOyd3KERUByr2jJQP0zAZIao7sXdJLdcauSWD0YSdZ65afv7lSDru+nALruaVyBgVETlaxUFrFrAS1Qf/soXPblyQNQxX8NX2c3hy8X6rfZduFGHW+mSZIiKiumDdM1I2tZcFrER1yD9Care8I28cLuCTTadt7r+az54RInfFFViJ6kPD5uXbmcfli8OJ6Q0mnL+aX+X9HKYhcjdcZ4SofnV9pnz7wi754nBSZ7Py0XHmJtz76fYqjzF34xKRe6g4TFOgLytg5WwaojrW6UmpLS2QNw4nk5lbjOf/dwiFZb+MKto6uS9ahPoBADpGB9VzZERUl1jASiQHpVpqDXp543Ayk35Iwpks6+GZTjFBOPf+/YgL8cXL/VsA4DANkbsRK3SNmP8Y4QqsRHXNp6HUcnqvRZHeiD3nrlXav2JcDygVAgAgqoE3AODc1XyrX15E5Nps/W9mAStRXTMXsV47K28cTiQx7YbN/V7q8r+O2kYGQKNSIDtfj5RsDnERuYub/7ZQKQRoVc731e98ERHdjqCya9TouAqr2bmsqmfPmGlVSrRvHAgAOHY5t65DIqJ6cnNPp49GCUEQZIqmakxGyL2otFLLmhGLSzeKKu1b+GSXSvsa+klL6ucVc0YNkbtyxuJVgMkIuRtzMmJkMgIApUYTjqdLPR3xEQEI8dPi/17sjYFtwysda57uV6g3QBRFHLhw3VJ9T0TuwRnrRQDAOaMiqi1l2QXzjJwVIooiEj7bYakBeXdYW3SJDa7yePMS0QUlRizbcwFv/3ISD3dqjM9GdKyPcInqzIbjmfDVKtGnRSO5Q6l3NxewOmsyUquekfnz5yMuLg5eXl7o0qULdu7cWe3xJSUlmDZtGmJjY6HVatGsWTMsWbKkVgETVcs8tddYKm8cTuByTpFVMWqbiMBqjzf3jGTkFmHuH2cAAOuOZSC3iOeSXNfZrHyM/98hPLVkPw5dvC53OPXvpmzEbYZpVq5ciYkTJ2LatGlITExEnz59MHjwYKSmplb5mOHDh+OPP/7A4sWLcerUKaxYsQKtW7e+rcCJbFKaa0bYM5KckWfZbhzkDW9N9WsLNC9b+OyHg5dwo1BKQPQGE95ccxx/ZerqLlCiOrTuaAYAwCQCE1cmIa/Ys5Lryj0jzrfGCFCLZGT27NkYM2YMxo4di/j4eMyZMwfR0dFYsGCBzeM3bNiA7du3Y/369RgwYACaNGmCbt26oVevXlW+RklJCXQ6ndWNqEa0/lIrGoGCbHljqUOXc4pgMJqqvF8URXyz47zl58e6Rt3yOYd2iESIn9by831tpKsg/3IkHYPm7MSwebux8kAq60jIpfx2XEpGVAoBadeL8Nb/nZA5ovpxIbsAszefrnSJB7cYptHr9Th06BASEhKs9ickJGDPnj02H7N27Vp07doVH330ERo3boyWLVti8uTJKCqqXOFvNmvWLAQGBlpu0dHR9oRJnkzrB3iVDUcU2V5fw9XtPpuN3h9sQf/ZVV9jZtupq9h/QeqSHtQ2HM/3bXbL5/VSK/HsXU0AAO2jAvH1k13w32e7YXC7cKgUApLScvDPn4+h+/t/YOqqYzh9Ja/6JySS2bmr+fgrMw8qhYCFT3aBQgBWJ17G/yW5/9T/hTvOWYZbK3LG69IAdhawZmdnw2g0IiwszGp/WFgYMjMzbT7m/Pnz2LVrF7y8vLB69WpkZ2fjhRdewPXr16usG5k6dSomTZpk+Vmn0zEhoZrT+APFucC5rUBIC7mjcYgPN/yFBdvOAQAa+kpFuhevFVZ5/O/J0gq0T/WMxdsPtavx6zzXpylC/LS4p2UjCIKAu1s2wt0tG+FqXgl+PnwJ3+9PxYVrhVixPxVrky7j0L/vs1o8jTzLoYs3kKUrxqB24U65dsX6siGa3s1D0D8+DC/f2wKf/3EGb64+js4xDRAd7CNzhHUnp9D2cJRb9IyY3fyhE0Wxyg+iyWSCIAj49ttv0a1bN9x///2YPXs2li1bVmXviFarRUBAgNWNqMbM03rdaOEzcyICANcKyqctl1YxVHMiXRra7Nqk6tkztqiUCgzvGo2wAC+r/Y38tRh/TzNsndwXK8b1gLdaiQK9EZdzqu7hJPf3/P8O4flvD+NzG3+BO4P1x6U/kofcEQEAePne5ugcE4S8EgMmrkxy6+Ls4tLKF8UEAD93qBkJCQmBUqms1AuSlZVVqbfELCIiAo0bN0ZgYHklf3x8PERRxKVLl2oRMtEtxA+V2syjdfoyoijig9/+wq9H0+v0daqz64x1Xcy5q/l459eTSErLAQC0DPN36OsJgoCezRparmWTzmTEY204noGssgsrzvn9DBbvSpE5Imsp2QVIztBBqRAs9U8qpQKfP94JfloVDl28gb4fb8V/9lyoMql3ZcWltt+TW/SMaDQadOnSBZs3b7bav3nz5ioLUnv37o309HTk55cvSX369GkoFApERd26qI7IbiEtpbaOC1i3nbqKr7afw0vfJQIAsvNL8NOhS2jz1gYs3pXisAvOVfeL8pllByzboiii/6fbrb4UGpclDY5m7t6uyVLz5J7G/++w1c/v/HoSPxxIAyB9Fl/87jAeXbDHkhjXt/XHpCGaXs0aokHZ0CYgfXaXPH0nmof64UZhKaavPYGEz3Zg44lMt7pIZFEVPSNukYwAwKRJk7Bo0SIsWbIEycnJePXVV5Gamorx48cDkOo9Ro8ebTl+5MiRaNiwIZ555hmcPHkSO3bswOuvv45nn30W3t5184uSPJy5TqSwbtcUOJlRPsuruNSIru/+jsk/HkGh3oh3fj2J13484pDXOVvDL/zfjleu26qrNQU6RQcBAA5cdM8iYbLPuD5xAIApq45i3dEMHLmUi3VHM3Dw4g08Mn83PtzwF0oMtr8c64LeYMKaRGmY9v6yIZqKusUFY8MrffDusHZo6KtBSnYB/rH8EEZ8vQ9HZEqeHK3qYRo3SUZGjBiBOXPmYObMmejYsSN27NiB9evXIzZWukBZRkaG1Zojfn5+2Lx5M3JyctC1a1eMGjUKQ4cOxdy5cx33LogqiugotbpLQEnNZ3xcyy/BH2WFn9XJzi/B5pNX8PHGU5Z9rf+9odJxqw5fxvK9F2A0SX9tnbmSh/9LuoyT6Tpk5hbj0MXrVf4ltvfcNfxf0mXkFZdi4fZzNo8xy8wtBoB6/SV6Z5xUi7Lj9FV8s+M80q5XXUxbF4r0RlzRFVf5C5fq17/uj8cT3aJhEoFXvk/Egm3SVbMb+KhhEqWapwfm7qqXz6goipi+9jjOZOXD30uFQTYufQBIQzZ/7xGLba/3xYv9mkGrUmB/ynU8NG83Xvk+sdKUWFdTYnCtYRpBdIF+KZ1Oh8DAQOTm5rKYlW5NFIF3QgCTARjxv/IakmofIiJu6nrLz/un9ceRtFxs+esKYhv6ls0uAQbNqX61YQBIaBOGLX9lwWC69X8tL7UC347tgS6xDSz7MnOL0WPWH7d8rNnGiXdjxtoT2Hv+mtX+9lGBWPvSXTV+HnsUlxrR+4MtVsW0bSMDMKhtOAa1C0fzUL86mV1xKjMP/917AasTL6NQLyUiXmoFGvhoEOitRpCPGkHeGgT5qBFYYTvI+6affdTwVjvn1UvrU4nBiMs3ihDb0BdKhX3nosmUdZbtCx8MgdEk4uH5u3H0UvlVnxeM6gyFQsC01ceQna+HUiFg/D1NMaF/C2hVdVNIuXzvBfz7/05AIQBLnr4TfVuF1uhx6TlF+GTjKawq61EZe1cc3nygTZ3EWB96zvoDGWV/qFT08/M9q70shKPV9PubyQi5p6/7AemHgX5vAve8Xu2hl24U4q4PtzrkZYe0j8C8kZ1xJC0HL3x7uNJsk/ZRgVa/rM0+/NsdGHFnDL7afg4f/PaXzef+9eW7cDJDhzd+kgpz4yMCkJyhw51NGuDABevhkqS3pCm3dTnt9mpeCX47noHfjmXiz5RrqJh7NW3ka0lM7mgceFtf+gajCb8nX8F/9ly0SrgEQco7a0ujUiCoQgITWJa0SMmKjeSm7Gc/rcolk5j8EgOSM3Q4fjkXJ9J1OJGuw5kreTCYRDzcqTFmD+9Q4/dlNIlo9q/y5P3CB0MAAFN+Porvy+pGvNVKHP73ffDWKHG9QI/pa0/glyNSsXerMH988lgH3BFV/SUK7LX33DU8ufhPGEwipg5ujX/cc+v1dW72y5F0vLwiEQ181Nj3r/51ljTVtU4zN1lWUq7ot1f6ID6i/r5Ha/r97Zz9NUS3645HpWTk4m4AVScjBSWGWyYi/loV8mrYZWvuEu4QHYTdU+7F1lNZeP5/h9AuMhAzH2qHNpEBKNIbsXRPCgpLjNh4IhNnsvLxz5+P4Z8/H6vyeR9oH4F2jQPRrnEgWof7o3moH9746SiSM3SVEpG1L/VGkI+mimdynEb+Wozu2QSjezbB9QI9fj95BRtOZGLXmWycv1qA+dvOYf62c+geF4zvn+th9xf4tfwSfH8gDd/uu4j0sr/wlAoBCW3CMLpnE3SPC0ZeiQG5haXIKdIjp7AUOUWlyC0s384pLEVukfXPOYV6GEwi9AYTsvJKLDNCakqlECokJxqbvS4V7zMnNP5eKihs9D7M3nwaPx5Mw/Cu0RjTJw4BXmq74qnq3JkTjuPpuTiZrsOFawVVJm+rEy/j7pYheLhTzSYVVCyqXvbMnZbtqAoF0/fGh1ouQRDsq8EXT3TCkDvCMW31cZy6kodh83djxbge6BbnmL/S064X4sXvDsNgEvFQx0g8d3fTWj3P4HbhCAvQ4opOGo59oH2kQ+KrKD2nCMMX7sXjd0bjpXvrZi2kqgpYnbVmxDmjIrpdTfpI7YVdQH4W4Fe5q/b45Vw88MUuq33T7o9HizA/rDp8GaO6x6B704YQRRFv/3ISy/ZcuOXLDm5nPT7dr1Uo/npnsNU+b40SL/RtDgB49q44dH7HenYaALzQtxleH9gKi3elYOOJTMvxANA+KgiA9KVoi/n++hTsq8HwO6Mx/M5o5BWXYuupq9hwPAPrj2Xiz5TryC8xwL+GX7JHL+Vg2Z4L+PVIBvRlX3rBvho80S0ao7rHIjKo/Asv0Fv64o9BzRevEkURhXpjWXJSlqhUSGhyK+4vKrUkOzcKS6E3mGAwicjO1yM7Xw+g4JavZ6YQgAY+GjzZMxav9G8BQRCQkVuE+VvPwmAS8fkfZ7BszwU8d3dTPN2rSY3G9kVRRHpusaW342S61NrqngeA8AAvtI0MkG6NA9E2MgA/H7qMz34/jbfWnEDX2GCbC4Ft+esKfk/Owov9mqNxkLdVPUKvZiGW7YqPHWKjcHRQuwh0i2uIF749hH3nr2PD8UyHJCOFegOeW34I1wv0aNc4AB/+rX2te6/Ma+18seUsVh5Iq5NkZM+5a7h0owif/X4GQ9pHIi7E16HPL4qiy03tdc6oiG5XRHsg6k7g0gHgwCKg378sd2XlFWP+1nOVkot5IztjSHvpF2jFcWZBEDDjwbbo2qQBWof7o1kjP0t9yZoXe2PUN/tQoDdi3YS7oFLaVxMe7KtBRKCX1ZfH8jHdLJc6H9unKcb2sf0XXsXpis7E30uNBztEYmj7CLR6cwP0RhN0xdUnI4V6A349koFv96daFTm2jwrEUz2bYEj7CIcNOQmCAF+tCr5aFRoH2Tejr7jUaJW4mHtebhTe1AtTsZemqBSFeiNMorRg3Zzfz+B6gR4zhrbF0t0XYDCJaBXmD6Mo4mxWPj7eeApLdqXg+b7N8PcesZb3bTSJSMkuwImyno7jZYlHVSttxoX4oo058YiUEo+K1x4ye7FfM+w4cxWHLt7ApB+S8P1zPa3qR5bsSsE7605CFIE/kq9g2TPd0Mi//HnUyvJjY8qSEW+1Ev2qqNUI9tVgcLsI7Dt/HZdzbr/wWRRFvP6j1EsY4qfF1092ve3PijkZ2XkmG2nXCx2+UmtOoVRrZTSJ+GTTKcwb2dmhz19V8SrgvBfKYzJC7qvzU1IycnoD0O9fuJxThN4fbLF56KNdoiyJSFUq/oVkHiMHgO1v9EN6ThHaRtZu/Hvlcz1xNb8YmbklaNrIt8bjue1svJ6/l/P8lxYEAQHeamTnl+B6vt7mF39yhg4r9qdi9eHLlqEwjVKBIe0j8FSvJuhYNoXYWXiplQgPVCI80OvWB1dQYjAit6gUG45nYvraE/jv3ovIKzZg80lp9tY/B7fCPS1DsfbIZcz5/QwuXivEu+uS8fWO87i3dSjOZOUjOUNnKdqtSKUQ0DzUD+3KejraRgYiPsK/xj1RKqUCnw3viPvn7sSBCzew+eQVDGoXDpNJxKzfkvHNTmndmiAfNa7oSjD8q714+6G2AKREpGIPRMfoILw6oCVahftXe5Vo82fBESv4ztt6FuuOZUCtFPDV3ztb9ZzVVnSwD+5qHoJdZ7Pxw8E0vJbQ6rafE5B6Y6etPoYjFerG1h3NwPi7cx1aP1NSRa+IWik4bQ2M8/zmInIgURSxKdMXAwEg40i1haGAVFBXWyF+Wpt/cdZUTEMfxDS0/y8vW7+8elfoMncG8RH+2HmmBEv3pGD28I4ApGm5vx5Nx3f7U5GYmmM5NibYB090i8FjXaNu63w6I61KiVB/JUb3bAJ/LxUm/3gUq8tmbbQI9UPflqFQKAQ83CkKD7SPxKrDlzD3j7O4nFNkKQgFpJlD8RFSb0e7yEC0jQxEizC/2+4JiGnog1E9YrBw+3n8dOgS+rVuhNd+OIJfy67t8sagVhjVLRbjlh/E/pTrmPSDtIaO5qaeQEEQ8MqAW9dANLas4Gt7OKkm0q4X4rPNpy2zX2Y+1M7uyx9U5/Fu0dh1Nhs/HryEV/q3sLvXsyKD0YSFO85jzu+nUWosL9xRKQQYTCI+2vgXlo/p7oiwAQDFVazp4qxDNACTEXJDGblF6DlrC1oJVzCw7Dvt89+SANj+a7ZbXDCe6d2kvsJzmGaN/Crte2uoc01FnJzQCjvPZGPV4cu4p2UjJKbmYNXhS9AVS70gKoWAhLZhGNktFr2aNbRZ4OluHu4UBS+VEhO+T0SpUcS4u5tavW+1UoERd8ZgWKfGWH34Mi5cK0TrcH+0axyAuBA/u6fg1tSjnaOwcPt5bD2VhVHf/ImDF29ArRTw0aPtLYWt/322Gyb9kIT1x6QF9tSq2n1Bm3svrhfoUag3wMeOK8leL9Bj3tazWL73oqWm6B/3NMUT3WJqFUtV7msThgY+amTqirH99FX0j7d9yZNbuZBdgEk/JOFwhcTb7O89YvHtnxex80w29pzNRq/mjvljoqr1d5z1ir0AkxFyM1l5xeg5SxqKOSWWX+k5XkjFYbElIgO9kNA2HNcK9Ggd7o8X+jZzyWmaACp9KY3qHuOQLmpH6hAdhEc6N8aqw5fxyvdJlv1RDbwtvSCh/vYNebiDwXdEYIW/Fkcv5eLRzrZnsGhVSjzu4C/Y6rQI80eH6CAcScvBwYs34KdVYeGTXdC7whekl1qJL57ojFB/qaA71L92PViB3mqE+GmRnV+CZXsuWBVoV6VQb8DS3Rfw1bZzliG9Xs0aYsrg1nVStK1VKfG3zlFYtCsFX+84b3cyIooivtufivfWJaNQb4SfVoUZD7bFwu3ncKZsVeX2UYEQxRj8Z+9FfLjxFNY0a+iQ30euNpMGYDJCbqDEYEResQEhflp0e6/iYmECdF6RCChOh69QjCF3RDi8UExufVqEYGfZxfKGdnB81b8jvDGwNbb8lYW8YgMGxIdiZPdY9Gke4hG9INXp2iTYocMKjjCiazSOpOUgLECLpU93Q5vIyvVLSoWA6UPb4N7Wobd17aM3BrbCGz8fxaebTqNH04boHNPA5nGlRhN+OJiGOb+fwdWyadhtIgIwZXBr9GkRUqd/TIzpE4f/7r2IP1OuY8+5bKuZQ9XJyivGP386iq2nrgIAuscF45PHOiA62Ac/Hiwfdmvgo8FL97bAj4cu4UhaDjaeyMSgdtXXrtVEVTNpfJy0eBVgMkIuThRFtHpTWoq9b6tGVvftnnIvAtY0Ay6kY3noCuCJKXKEWKeWPH0nLt0ogtFkQvNQx16h11HCA72w5bW+AKSZFOS8Hr8zGsG+anSJDbaaMXMzQRBwd8tGVd5fE491jcLOs9n45Ug6JqxIxPpX+litsSKKIjYcz8THG0/hfLY0hTo62BuTE1phaPvIeklmIwK98Xi3aPx370XM2XwGPZveuufit2MZ+NfqY7hRWAqNUoHXB7bCmLviLPFW/D8Q6KNGI38txt4Vh7lbzuLjjacwID7MUp+iKy5FfrHB7h5PV7suDVCLa9O4K1GUFkEqNZrc6sqNzsxkEnHpRmGtz7fBaLJawn1b2V8hSoWA5JmDpIr9zmUXbcy5COTf+rozrkatVCAuxNdpExGzYF8NExEXoFAIGNQuotpExFEEQcB7D7dDdLA3Lt0owr9WHbP8Lth3/hqGzd+D5789jPPZBQj21WDG0Db4Y1JfPNSxcb32qr3Qtzk0KgX2X7iOPeeuVXmcrrgUk35IwvPfHsaNwlLERwTgl5fvqlQTVPH/QZC3lHyNvbspGvioce5qAVYdvmy5/6kl+9H3k204ZmPV5uqwZsTFvP7jEaw/loFSo2gphDITBKm4TqkQoBTKWoUApUIBpQJQKRQV9knHVJUwV/yuFSFWc1/F/aLN/TfvqOoxle+zfgofjRIB3moEeKng7yW1CoUAvcGE8AAvhAd6wWgSEeSjQUM/DTRKBUoMJpQYjFArFfDTquClVqCgxAgfjRJXdCUQIaJdZCDWHknHFV0xVAoBf+8Zi0Z+Wqu/JowmEUPm7sRfmdYXsXvv4XYY1T0WeoPJck2Ypo18bU5Fy8orvmlIppzRJJZPK2w/HFj3GlCiA479CPR62eZjiKj+BXipMffxTnjsq7349WgGmjXyw9FLOZbhDR+NEmP7NMW4PnE1nqrsaOGBXhjZLQbL9lzAZ5tPo5eNuo4957Lx+o9HcTmnCAoBGH9PM0wc0BIaGwW+Fd+HeaXkAC81XuzXHO+uS8Znv5/Ggx0joVUpcPxyLkqNIqatOYbVL/SucfGyqy14Bnh4MlJqNKHAxrx9QPryLjWKVtOwyH5zt5yt8bHTVh/HtNXHbd73SOfGuJpXgs4xDbD1VFal67uM6BqNlWVjsZ8/3tH6wS0HAcd+APZ8AXR7DlC517RRIlfWKaYBJiW0xEcbTuHzP84AkP4QfKJbDF7u39wpCpyf79sM3+1PxcGLN7DrbLZlUcLiUiM+3ngKi3dJa7HEBPtg9vAO1dYCVVw5OaDCukB/7xGLJbtSkJ5bjP/tu4hHOkdZvn+OXsrFd/tT8WSP2BrFW/UwDWtGnNLU++Px6n0toVYqoFYqoFEqIEKE0VR2E0UYjOXblv1lN8NNP5tE0ap3RED5D9b7YfOHmhxfMSOv+hjbL2DeL4oiCkqM0BWXIq/YgLziUuiKDDCYRCgE4MK1AuQUlsJLrcSNQj2uF+hRajDBS62ERqWA3mBCfokBJQYTvNVKZOpqv1bA4Hbh+O145i2PM3ddmos1K/rnoNYY2ycOXZs0QPe4hpXX7Bj8IXDy/6Rhmt1zb3nhPCKqX+PvboYDKdex9dRVDGkfgckJrRy+RPrtCAso7x2Z8/sZ3NU8BCfSdZj0QxJOX5FmxjzRLRpvDmlzy96Hip0bFdcu8VJLvUAzfz2JHWeycVcL62LZjzb8hUFtw2s0hFblMA17RpxTWID8Gbc70BWXIjuvBI38y4dj/LQqXM4pwvvrkrHuWEalx/z68l1o17h80a5SownvrUtGVANvxDb0xR2NA+GjVWL6/52wLA51s91T7rVa1fOxrtE2j4NPMBAcB1z9C9j6LpMRIiejUAhY9NSdyCnUo6GTLnj3Qt9mWLE/FYcu3sBrPx7BL0fSUWoUEeKnxYd/u6PGU38DvKsebjIvZHguK98yc6h5qB+81Aocv6zD++uT8dmIjrd8DSYj5JECvNQ2rzTaOMgb80Z1xrwK+wxGk82VDNVKBWY82LbS/s9GdMRnIzpCFEUIgoCjl3KQnlNk//S3rmOA38qSkLxMwD+8+uOJqF4pFYLTJiIAEBrghVHdY7Fkd4qlp3Zg2zC8//AddsX9RLcYbDyRiQE2kpfmZQsZXs4pwoVr0nV7IgK9MDmhFYbN343ViZfxWNeoW04xLq7i2jS+1SzRLzfOpqF6Vdsllc09Lu2jgmo3D7/bOCCgsbT93fBaxUBEnm1836Zo6KuBn1aFTx7rgK/+3sXuBMpXq8KP43vhH/c0q3RfA18NGpbNttl3Xpq508hPiw7RQRjVXVoA799rjkNfzYXwgPKeEe1NBbTO3DPCZIQ8gyAAjcsWPMs4Unl6ERHRLYT6e+GP1+7Bn//qj0e7RNXJgmvmyzzsK5tG3ChASnZeH9gaIX4anLtagG92nq/2OcwrsN58jSeuM0LkDIbOLd8u0ckXBxG5rCAfTZ32MDQLlZKRawV6ALDMJgr0VmPakHgAwBdbziDtemGVz2G+am/ITcWu7BkhcgbeDQCh7COvr/o/MhGRXJqHWl8As+LsmWEdG6NH02AUl5owY+2JKheMNA/ThNy00CCTESJnIAiAqmz2zbUz8sZCRGTDzclIxYsRCoKAd4e1g1op4I+/srD5pO1VpYs5TEPk5BqULRqUaXtxNSIiOTVrZL2+ys1XRm4e6o9xfZoCAN7+5SQK9YZKz1FsGaa5uWeEs2mInEO7R6T28kF54yAisiEy0Bve6vKkIdTGelgv39sCUQ28cTmnyLJqbUXFBqlnpKEve0aInFMjqQAMmcfkjYOIyAaFQkCzUKl3xFuttLk2iLdGibfL1mVavDMFp266zldR2WVOGvpZ94z4OPGF8piMkGeJ6SG12aeBYs6oISLnY178LDRAW+X04f7xYbivTRgMJhH/XnPcqpjVvOhZxR4WADYv3OcsnDcyorrgGwJoy5ahz6u8TD0RkdzMa43cXC9ysxkPtoW3Won9F67jl6PS77MZa0/gSFoOAJRfvdwFMBkhzxMQKbU629e8ISKS090tG0GtFNDzFsu+Nw7yxlO9mgAAtp+6CgBYtueC5X6tSolucdIVhM2ts3LeASSiuhIQCVxNBnTpckdCRFRJh+ggHJsxEF7qW/dsdCi7uN7ZrLxK92lUCqx8rgfySwxOXbwKMBkhT2TpGWEyQkTOqSaJCAC0CJOGdM5m5VdaBE2rUkAQBPjbuJCps+EwDXkeJiNE5CZiG/pCpRBQoDfi0o0iq/ucuWD1Zq4TKZGjMBkhIjehVioQFyJNBT6Rbj1D8Oar9joz14mUyFECGkstkxEicgPmoZoT6blW+9kzQuTM/MKkNj9T3jiIiBygeag/AOD4ZetkRKvk1F4i5+VTNsWt4CpQxVUviYhcRYuyi+sdv3mYRu06X/GuEymRo3hXmG9fdEO+OIiIHMA8THM1r8Rqv0bpOl/xrhMpkaNofMq3mYwQkYuLC/GFwsaq8QpbO50UkxHyTAFRUlucI2sYRES3S6tSIrahr9xh3BYmI+SZvMquT8OL5RGRG2heVjfiqpiMkGfyCpDa4tzqjyMicgEtmIwQuSBzz0gJe0aIyPWZi1hdFZMR8kxa9owQkftoUbbWiKtiMkKeiTUjRORGmjXyg+A6k2cqYTJCnok1I0TkRrw1SkQ18JY7jFpjMkKeiTUjRORmXHmohskIeSbWjBCRm3HlGTVMRsgzsWaEiNzMiDuj5Q6h1piMkGdizQgRuZmmjdgzQuRavBtILa9NQ0QkOyYj5Jk0ZYVepQXyxkFERExGyEOptFJbWixvHERExGSEPJS6bD6+oQgQRXljISJykNnDOwAA3nu4ncyR2EcldwBEstBUuNy2Ph/Quu78fCIis0c6R2FQu3D4aFzr6509I+SZNL6ApqzyPD9L3liIiBzI1RIRgMkIeTK/MKnNy5A3DiIiD8dkhDyXf7jU5mXKGwcRkYdjMkKey9wzkn9F3jiIiDxcrZKR+fPnIy4uDl5eXujSpQt27txZo8ft3r0bKpUKHTt2rM3LEjkWe0aIiJyC3cnIypUrMXHiREybNg2JiYno06cPBg8ejNTU1Gofl5ubi9GjR6N///61DpbIodgzQkTkFOxORmbPno0xY8Zg7NixiI+Px5w5cxAdHY0FCxZU+7h//OMfGDlyJHr27FnrYIkcij0jREROwa5kRK/X49ChQ0hISLDan5CQgD179lT5uKVLl+LcuXOYPn16jV6npKQEOp3O6kbkcExGiIicgl3JSHZ2NoxGI8LCwqz2h4WFITPT9i/0M2fOYMqUKfj222+hUtVs7vOsWbMQGBhouUVHu+5lkcmJ+TSUWl4sj4hIVrUqYBUEwepnURQr7QMAo9GIkSNH4u2330bLli1r/PxTp05Fbm6u5ZaWllabMImqpyy7Po2xRN44iIg8nF3LtIWEhECpVFbqBcnKyqrUWwIAeXl5OHjwIBITE/HSSy8BAEwmE0RRhEqlwqZNm3DvvfdWepxWq4VWq7UnNCL7qTRSa9DLGwcRkYezq2dEo9GgS5cu2Lx5s9X+zZs3o1evXpWODwgIwLFjx5CUlGS5jR8/Hq1atUJSUhK6d+9+e9ET3Q72jBAROQW7F7CfNGkSnnzySXTt2hU9e/bE119/jdTUVIwfPx6ANMRy+fJl/Pe//4VCoUC7dtZXDgwNDYWXl1el/UT1zisAgACIJiD/KuDXSO6IiIg8kt3JyIgRI3Dt2jXMnDkTGRkZaNeuHdavX4/Y2FgAQEZGxi3XHCFyChpfaa2R/EwgL53JCBGRTARRFEW5g7gVnU6HwMBA5ObmIiAgQO5wyJ3M7wlknQSeXA00q1y/REREtVfT729em4Y8m3l6b+F1eeMgIvJgTEbIs3kHSW3aflnDICLyZExGyLOV5EutjXVyiIiofjAZIc8WP1Rqr52VNw4iIg/GZIQ8W0RHqU1PApy/lpuIyC0xGSHPFtYGEJRAYTaQlyF3NEREHonJCHk2tTfQqLW0nXFE3liIiDwUkxGiiPZSm3lM3jiIiDwUkxGihs2kNvu0vHEQEXkoJiNEoW2k9tiPLGIlIpIBkxGi2ApXnObiZ0RE9Y7JCJF3A6Dtw9L2qrHyxkJE5IGYjBABQOsHpDYnFci9LG8sREQehskIEQC0+1v59t558sVBROSBmIwQAdK1afpNk7b3zQNS98kbDxGRB2EyQmR216vl21vfly8OIiIPw2SEyEypBp75TdpO2Q7MCAIWJwBHVpZf3ZeIiByOyQhRRbG9gHv+WfaDCKT9Cax+Dvi0FZD8i6yhERG5K0EUnX+VJ51Oh8DAQOTm5iIgIEDucMjdiSJwIwU4twXISQOO/gDkpQMQgKd/BZrcJXeEREQuoabf36p6jInINQgCENxUugFSLckPo6Whm0P/YTJCRORgHKYhuhXvIKD3K9L2sR9kDYWIyB0xGSGqidD48u30JNnCICJyR0xGiGoiILJ8+8+v5IuDiMgNMRkhqqknvpfa0xt4dV8iIgdiMkJUU7G9pbboBnD1L3ljISJyI0xGiGrKK6B8hs2VE/LGQkTkRpiMENnDK1Bqz2yWNw4iIjfCZITIHh1GSu3VZHnjICJyI0xGiOwR1UVqb1wETCZ5YyEichNMRojsEd4BUKiA4hxAd1nuaIiI3AKTESJ7KFXla46k7JA3FiIiN8FkhMheSq3UFmbLGwcRkZtgMkJkr/ihUnv9vLxxEBG5CSYjRPYKipbavCvyxkFE5CaYjBDZS1CWbXBJeCIiR2AyQmQvoey/zaWD8sZBROQmmIwQ2UsQpNZYKm8cRERugskIkb1iekptaSEXPiMicgAmI0T2CoqV6kZMpUA+i1iJiG4XkxEie1Vc+Cw3Td5YiIjcAJMRotoILJvem3lM3jiIiNwAkxGi2giIkNqzv8sbBxGRG2AyQlQbwc2ktiRP3jiIiNwAkxGi2mjWT2pZM0JEdNuYjBDVRlCM1N64AOgLZQ2FiMjVMRkhqg3/iPLtK8fli4OIyA0wGSGqDYUSaNxV2s69JG8sREQujskIUW01iJVaXbq8cRARuTgmI0S15R0stcW58sZBROTimIwQ1ZbaS2oNxfLGQUTk4piMENWWqiwZKbwmbxxERC6OyQhRbSk1Unv5sLxxEBG5OCYjRLWl8ZVa7yBZwyAicnVMRohqK/wOqeUwDRHRbWEyQlRbPiFSy2SEiOi2MBkhqi1fczJyHTAZ5Y2FiMiF1SoZmT9/PuLi4uDl5YUuXbpg586dVR67atUq3HfffWjUqBECAgLQs2dPbNy4sdYBEzkN8zojEKWEhIiIasXuZGTlypWYOHEipk2bhsTERPTp0weDBw9GamqqzeN37NiB++67D+vXr8ehQ4fQr18/DB06FImJibcdPJGslKryhKTgqryxEBG5MEEURdGeB3Tv3h2dO3fGggULLPvi4+MxbNgwzJo1q0bP0bZtW4wYMQJvvfVWjY7X6XQIDAxEbm4uAgIC7AmXqG592Q3IPgWMXgs0vUfuaIiInEpNv7/t6hnR6/U4dOgQEhISrPYnJCRgz549NXoOk8mEvLw8BAcHV3lMSUkJdDqd1Y3IKfk2klr2jBAR1ZpdyUh2djaMRiPCwsKs9oeFhSEzM7NGz/Hpp5+ioKAAw4cPr/KYWbNmITAw0HKLjo62J0yi+uNTllRfPy9vHERELqxWBayCIFj9LIpipX22rFixAjNmzMDKlSsRGhpa5XFTp05Fbm6u5ZaWllabMInqXtO+Unuh6iJuIiKqnsqeg0NCQqBUKiv1gmRlZVXqLbnZypUrMWbMGPz4448YMGBAtcdqtVpotVp7QiOSR2wvqb10EDCWAkq1vPEQEbkgu3pGNBoNunTpgs2bN1vt37x5M3r16lXl41asWIGnn34a3333HYYMGVK7SImcUUgrwCsIKC0EzmySOxoiIpdk9zDNpEmTsGjRIixZsgTJycl49dVXkZqaivHjxwOQhlhGjx5tOX7FihUYPXo0Pv30U/To0QOZmZnIzMxEbm6u494FkVwUCiC6m7R9ar28sRARuSi7k5ERI0Zgzpw5mDlzJjp27IgdO3Zg/fr1iI2NBQBkZGRYrTmycOFCGAwGvPjii4iIiLDcXnnlFce9CyI5tR8htac3ciVWIqJasHudETlwnRFyasZS4KNmQEku8OxGIKaH3BERETmFOllnhIhsUKqBlmVr7+ycLW8sREQuiMkIkSO0fkBqL+4GDCXyxkJE5GKYjBA5QvxQqdXnA1/1AfK5IisRUU0xGSFyBIUS6PmStJ19Cvi8PXD5kLwxERG5CCYjRI5y3ztArwnSdmkh8M29QNoBeWMiInIBTEaIHEWhABLeAV46CAhKad/iAcC2DwFdhryxERE5MU7tJaoLF/cA618Hrhwv3+cfIa3W2iAWCG4KdHsOCI6TLUQiorpW0+9vJiNEdcVYChxaBmx6EzAUV75fUAL3vQ30eFHqVSEicjNMRoicRUk+cOUEABHQFwDpiUDSd8D1c9L9jVoDQ+cCMd1lDZOIyNGYjBA5M1EE/vwK2DpLWrnVKwj45wVAEOSOjIjIYbgCK5EzEwSgx/PAK0mAUgMU5wBf9wXyMmUOjIio/jEZIZKTTzBw77+l7Ywk4NNWwOy2wIongL3zZQ2NiKi+MBkhklvvCcBz24DAGOln3SXg1Hpg41Tgt39KQzpERG6MNSNEzkIUgdw0IPM48P0T5fvj7pGmAbe4D1Bp5YuPiMhOrBkhcjWCAATFAK3vB966Xr6aa8p2YOUo4I+Z8sZHRFRHmIwQOSOFUlrN9el1gNpH2pf0HYdsiMgtMRkhcmZN7gLeSAEEBVB0nbNtiMgtMRkhcnZqLyC4mbSddVLeWIiI6gCTESJXENZGarOS5Y2DiKgOMBkhcgVh7aQ28X/S8vJERG6EyQiRK+j0JOAXBlxNBpJ/kTsaIiKHYjJC5AoCIoA2w6Rt1o0QkZthMkLkKhq1ktqMJFnDICJyNJXcATiS0WhEaWmp3GFQDanVaiiVSrnDcB2xvaU2ZYc0VBM/VN54iIgcxC2SEVEUkZmZiZycHLlDITsFBQUhPDwcgiDIHYrzC20NdBwFJH0LbHmPyQgRuQ23SEbMiUhoaCh8fHz4xeYCRFFEYWEhsrKyAAAREREyR+Qi+rwmJSM5F+WOhIjIYVw+GTEajZZEpGHDhnKHQ3bw9vYGAGRlZSE0NJRDNjXh3UBqSwsBXYZU2EpE5OJcvoDVXCPi4+MjcyRUG+Z/N9b61JBXUPn25UOyhUFE5Egun4yYcWjGNfHfzU4KBRB3t7TNoRoichNuk4wQeQzzUM3h/8obBxGRgzAZ8TBNmjTBnDlzHH4s1aOYnlKrL5A3DiIiB2EyIqOnn34agiBAEASo1Wo0bdoUkydPRkFB3X3JHDhwAM8995zDj6V61H6E1Oam8To1ROQWXH42jasbNGgQli5ditLSUuzcuRNjx45FQUEBFixYYHVcaWkp1Gr1bb9eo0aN6uRYqkc+wYBvI6DgKnDlOBDTQ+6IiIhui9v1jIiiiEK9QZabKIp2x6vVahEeHo7o6GiMHDkSo0aNwpo1azBjxgx07NgRS5YsQdOmTaHVaiGKInJzc/Hcc88hNDQUAQEBuPfee3HkyBGr51y7di26du0KLy8vhISE4JFHHrHcd/PQy4wZMxATEwOtVovIyEhMmDChymNTU1Px0EMPwc/PDwEBARg+fDiuXLli9VwdO3bE8uXL0aRJEwQGBuLxxx9HXl6e3eeFbqFpX6nd+C9ZwyAicgS36xkpKjWizVsbZXntkzMHwkdze6fU29vbMs317Nmz+OGHH/Dzzz9b1uAYMmQIgoODsX79egQGBmLhwoXo378/Tp8+jeDgYKxbtw6PPPIIpk2bhuXLl0Ov12PdunU2X+unn37CZ599hu+//x5t27ZFZmZmpcTGTBRFDBs2DL6+vti+fTsMBgNeeOEFjBgxAtu2bbMcd+7cOaxZswa//vorbty4geHDh+ODDz7Ae++9d1vnhW7S+Sng2I9A5jGpdkTjK3dERES15nbJiCvbv38/vvvuO/Tv3x8AoNfrsXz5cstwyZYtW3Ds2DFkZWVBq9UCAD755BOsWbMGP/30E5577jm89957ePzxx/H2229bnrdDhw42Xy81NRXh4eEYMGAA1Go1YmJi0K1bN5vH/v777zh69ChSUlIQHR0NAFi+fDnatm2LAwcO4M477wQAmEwmLFu2DP7+/gCAJ598En/88QeTEUdrchfgHwHkZQAn1wIdn5A7IiKiWnO7ZMRbrcTJmQNle217/frrr/Dz84PBYEBpaSkeeughfPHFF5g/fz5iY2Ot6jYOHTqE/Pz8SivNFhUV4dy5cwCApKQkjBs3rkav/dhjj2HOnDlo2rQpBg0ahPvvvx9Dhw6FSlX5Y5GcnIzo6GhLIgIAbdq0QVBQEJKTky3JSJMmTSyJCCAt825e8p0cSBCA1kOAA4uA9ZOBNg8BGi78R0Suye2SEUEQbnuopD7169cPCxYsgFqtRmRkpFWRqq+vdde7yWRCRESE1bCIWVBQEIDyJdZrIjo6GqdOncLmzZvx+++/44UXXsDHH3+M7du3VyqWFUXR5gJlN++/+XGCIMBkMtU4JrLD3a8DR38ASnTA1/cALx2QOyIiolpxuwJWV+Pr64vmzZsjNjb2lrNlOnfujMzMTKhUKjRv3tzqFhISAgBo3749/vjjjxq/vre3Nx588EHMnTsX27Ztw969e3Hs2LFKx7Vp0wapqalIS0uz7Dt58iRyc3MRHx9f49cjB/IPL5/mm30aSPxW3niIiGqJyYgLGTBgAHr27Ilhw4Zh48aNuHDhAvbs2YM333wTBw8eBABMnz4dK1aswPTp05GcnIxjx47ho48+svl8y5Ytw+LFi3H8+HGcP38ey5cvh7e3N2JjY22+dvv27TFq1CgcPnwY+/fvx+jRo3HPPfega9eudfq+qRqDK/zb/vYGYNDLFwsRUS0xGXEhgiBg/fr1uPvuu/Hss8+iZcuWePzxx3HhwgWEhYUBAPr27Ysff/wRa9euRceOHXHvvffizz//tPl8QUFB+Oabb9C7d29Lj8ovv/xi8+rHgiBgzZo1aNCgAe6++24MGDAATZs2xcqVK+v0PdMtKBTAPy9I2/p8YMdHgKFE1pCIiOwliLVZHKOe6XQ6BAYGIjc3FwEBAVb3FRcXIyUlBXFxcfDy8pIpQqot/vs5yOrxwJEV0nb7x4FHFsobDxERqv/+rsh1Kj2JqGrdxwMn/w8oLQSOfg9cPw9Ed5Nm3UR2Bto9cuvnICKSCZMRIncQ2VEarlnQG7h2Bri0X7qZ/fQM4BcG9J0CdH1WriiJiGxiMkLkLlRa4OWDwOVDQNp+QJcO7Jlbfn/+FeDXV4HcS0C/N6V6EyIiJ8BkhMjdNO4i3QDgvplSUiKagH0LgH3zgJ2fAnu+AFoOAga+DwRFV/98RER1jH8aEbkzQQACG0sJx6D3gUe+AdQ+gFEPJK8F5nYClj8C5F259XMREdURJiNEnqT9cGDyaWDQB9LPplLg3B/Apy2BLe8CXC2XiGTAZITI02j9gR7PA5OSgXvfLN+/42PgwDfyxUVEHos1I0SeKiBSur5Nl2eB/z0MZByRVnHNPg3E9gIEBRDZCQiIApT8VUFEdYe/YYg8nW9DYOwfwHfDgXNbpCsBH1hkfYxSK10VWOMn1ZxofIGoO4FOo6R9Kq+ym1ZqmbwQkR34G8PDNWnSBBMnTsTEiRMBSMu+r169GsOGDZM1LqpnSjUw/L/A/q+B1H1AaZG0vHzGEWkmjrEEKCoBim6UPyb9MLC/ipVeFary5CS4qXRBP99GUkGtIwkKAILUCgrp+S37BOt9VsfevE+o4rg6eE61t+PPA5GLYzIio6effhr/+c9/AABKpRKRkZEYMmQI3n//fTRo0EDm6MjjaP2BPq9Z7zOUAPoCKTHRFwKlBVJ77Qxw9EepNZRIyYuptPxxJkPZY/KBwmvApQP1+16cmVID+DQEfEIAr8DypEXtU37TVNhWaQDUYfIiCFKiGNAYaBALBMUyWaJ6x2REZoMGDcLSpUthMBhw8uRJPPvss8jJycGKFSvkDo2obNhFC/gEW++P61N5JVeTUUpMDMVlbRGQfRa4sFOqQynOdWxsoghAlHpuRHNrqrAPNvaJN/1c4Tirfbdx3K0Y9UBehnRzRtpAKTG9maAAvAIAbUB5GxwnDdd5Nyi/eQUCCmX9x00uzf2SEVGUrs8hB7WP3X9RaLVahIeHAwCioqIwYsQILFu2zHL/0qVL8dFHHyElJQVNmjTBhAkT8MILL1juv3TpEiZPnoxNmzahpKQE8fHxmDdvHrp3745z585h0qRJ2LdvHwoKChAfH49Zs2ZhwIABDnm7RFYUyrK6Ep/yfcFNgZYJ8sUkB1G8KVGqmCwZgaIcoDAbKLgGlOjKHmOSfm9V7H0qLbsZ9HUcr1FanVeXDty4AJTkSjdbapJPavylyxMolNJwndq7rJenYusNeAUBTfoA/uFSYsMVgT2a+yUjpYXA+5HyvPa/0qXCvlo6f/48NmzYALVaDQD45ptvMH36dHz55Zfo1KkTEhMTMW7cOPj6+uKpp55Cfn4+7rnnHjRu3Bhr165FeHg4Dh8+DFPZWhH5+fm4//778e6778LLywv/+c9/MHToUJw6dQoxMTEOectEdBPzsAsAwEYPgdbfeVe9NeiBa2elGqGbmYxS8lScCxTrgKLrwMU9Ug9PUa5UT6TPk24Xdtr3uoJC6lHR+qPOhqTU3tLwmHcDqafPpyHgHSxte5f9bN72DmLvTj1zv2TExfz666/w8/OD0WhEcXExAGD27NkAgHfeeQeffvopHnlEuuJqXFwcTp48iYULF+Kpp57Cd999h6tXr+LAgQMIDpa60Zs3b2557g4dOqBDhw6Wn999912sXr0aa9euxUsvvVRfb5GIXIVKA4S1qfnxd71q/bNBD6TtK1/R11Qq1RNZboXl7dW/gIyj0nCeaJKSmYoF0nISFFJy4hcm1dN4N5ASFK9AqUdH7X17zx8YBTRqXT4zTePn8T1DtUpG5s+fj48//hgZGRlo27Yt5syZgz59+lR5/Pbt2zFp0iScOHECkZGReOONNzB+/PhaB10ttY/UQyEHtc+tj7lJv379sGDBAhQWFmLRokU4ffo0Xn75ZVy9ehVpaWkYM2YMxo0bZzneYDAgMDAQAJCUlIROnTpZEpGbFRQU4O2338avv/6K9PR0GAwGFBUVITU1tXbvj4ioOioNEHe3fY8pLQaKc6Thq5K8uogKgCgVYhddBwqvS0lP4XWpuNqy7zpQeEMaohJNQMFV6VZf1GVT57V+UrIS3LRs6F9RPmW+YqvUlv+s9irr8QmRkiiN/d9FcrM7GVm5ciUmTpyI+fPno3fv3li4cCEGDx6MkydP2uz6T0lJwf33349x48bhf//7H3bv3o0XXngBjRo1wt/+9jeHvAkrgnBbQyX1zdfX19KbMXfuXPTr1w9vv/22pefim2++Qffu3a0eo1RK3Yfe3tVn56+//jo2btyITz75BM2bN4e3tzceffRR6PV1PAZNRFRTai9AHS7VjjgDY6mUnBRkSbU0+VlSolScW5402RrGqimTEUhPkpKf0sLyomdzjVBBFnD9PJCyo/av4RcORHSQpuybE5vY3lLhcXV8Gsr2/Wl3MjJ79myMGTMGY8eOBQDMmTMHGzduxIIFCzBr1qxKx3/11VeIiYnBnDlzAADx8fE4ePAgPvnkkyqTkZKSEpSUlP9j63Q6e8N0WdOnT8fgwYPx/PPPo3Hjxjh//jxGjRpl89j27dtj0aJFuH79us3ekZ07d+Lpp5/Gww8/DECqIblw4UJdhk9E5NqUasA/TLrhjrp9LVGUZp/pC6ReIX2BlPTkpErFxEa9VGBs0FeYpVahNZbt1xeW9fZkS/vyM4EzmfbH87fFwB2POvxt1oRdyYher8ehQ4cwZcoUq/0JCQnYs2ePzcfs3bsXCQnW1fQDBw7E4sWLUVpaainWrGjWrFl4++237QnNbfTt2xdt27bF+++/jxkzZmDChAkICAjA4MGDUVJSgoMHD+LGjRuYNGkSnnjiCbz//vsYNmwYZs2ahYiICCQmJiIyMhI9e/ZE8+bNsWrVKgwdOhSCIODf//63pbiViIhkJgjls4t8Qyrc0bt2zyeKQEE2kLK9bCZWiZTUnN4I5Kbd+vEyFu3alYxkZ2fDaDQiLCzMan9YWBgyM21nYZmZmTaPNxgMyM7ORkRERKXHTJ06FZMmTbL8rNPpEB3tpNXndWDSpEl45plncPbsWSxatAgff/wx3njjDfj6+uKOO+6wrJaq0WiwadMmvPbaa7j//vthMBjQpk0bzJs3DwDw2Wef4dlnn0WvXr0QEhKCf/7znx7Vy0RE5FEEAfBrVLl3Y+B78sRjh1oVsAo3raUhimKlfbc63tZ+M61WC61WW5vQXErF9UQqGjlyJEaOHFlp25bY2Fj89NNPNu9r0qQJtmzZYrXvxRdftPr55mEb878NERFRfbFrLlFISAiUSmWlXpCsrKxKvR9m4eHhNo9XqVRo2LChneESERGRu7ErGdFoNOjSpQs2b95stX/z5s3o1auXzcf07Nmz0vGbNm1C165dbdaLEBERkWexe5WVSZMmYdGiRViyZAmSk5Px6quvIjU11bJuyNSpUzF69GjL8ePHj8fFixcxadIkJCcnY8mSJVi8eDEmT57suHdBRERELsvumpERI0bg2rVrmDlzJjIyMtCuXTusX78esbGxAICMjAyrRbXi4uKwfv16vPrqq5g3bx4iIyMxd+7culljhIiIiFyOILpAxaJOp0NgYCByc3MREGC9aEtxcbHlInK3WgSMnE9RUREuXLiAuLg4eHl5yR0OERE5UHXf3xW5/GL45rqTwkKZrtRLt8X878b6ISIiz+XyF8pTKpUICgpCVlYWAMDHx6faacbkHERRRGFhIbKyshAUFGRZ4p6IiDyPyycjgDR9GIAlISHXERQUZPn3IyIiz+QWyYggCIiIiEBoaChKS0vlDodqSK1Ws0eEiIjcIxkxUyqV/HIjIiJyMS5fwEpERESujckIERERyYrJCBEREcnKJWpGzOuy6XQ6mSMhIiKimjJ/b99qfVWXSEby8vIAANHR0TJHQkRERPbKy8tDYGBglfe7xHLwJpMJ6enp8Pf3r9WCZjqdDtHR0UhLS6t2OVpPx/NUczxXNcPzVDM8TzXD81QzznSeRFFEXl4eIiMjoVBUXRniEj0jCoUCUVFRt/08AQEBsv/DuAKep5rjuaoZnqea4XmqGZ6nmnGW81Rdj4gZC1iJiIhIVkxGiIiISFYekYxotVpMnz4dWq1W7lCcGs9TzfFc1QzPU83wPNUMz1PNuOJ5cokCViIiInJfHtEzQkRERM6LyQgRERHJiskIERERyYrJCBEREcnKZZOR+fPnIy4uDl5eXujSpQt27txZo8ft3r0bKpUKHTt2rHTfzz//jDZt2kCr1aJNmzZYvXq1g6Ouf44+T8uWLYMgCJVuxcXFdRB9/bHnPG3bts3mOfjrr7+sjvP0z1NNzhM/T5KSkhJMmzYNsbGx0Gq1aNasGZYsWWJ1jKd/noBbnyd3/TwB9p2rp59+2uZ5aNu2rdVxTvWZEl3Q999/L6rVavGbb74RT548Kb7yyiuir6+vePHixWofl5OTIzZt2lRMSEgQO3ToYHXfnj17RKVSKb7//vticnKy+P7774sqlUrct29fHb6TulUX52np0qViQECAmJGRYXVzZfaep61bt4oAxFOnTlmdA4PBYDmGn6eanSd+niQPPvig2L17d3Hz5s1iSkqK+Oeff4q7d++23M/Pk+RW58kdP0+iaP+5ysnJsXr/aWlpYnBwsDh9+nTLMc72mXLJZKRbt27i+PHjrfa1bt1anDJlSrWPGzFihPjmm2+K06dPr/QlO3z4cHHQoEFW+wYOHCg+/vjjDolZDnVxnpYuXSoGBgY6OFJ52XuezF+yN27cqPI5+Xmq2Xni50kUf/vtNzEwMFC8du1alc/Jz1PNzpM7fp5Esfa/y81Wr14tCoIgXrhwwbLP2T5TLjdMo9frcejQISQkJFjtT0hIwJ49e6p83NKlS3Hu3DlMnz7d5v179+6t9JwDBw6s9jmdWV2dJwDIz89HbGwsoqKi8MADDyAxMdFhcde32p4nAOjUqRMiIiLQv39/bN261eo+fp7KVXeeAH6e1q5di65du+Kjjz5C48aN0bJlS0yePBlFRUWWY/h5qtl5Atzr8wTc3v89s8WLF2PAgAGIjY217HO2z5RLXCivouzsbBiNRoSFhVntDwsLQ2Zmps3HnDlzBlOmTMHOnTuhUtl+y5mZmXY9p7Orq/PUunVrLFu2DHfccQd0Oh0+//xz9O7dG0eOHEGLFi0c/j7qWm3OU0REBL7++mt06dIFJSUlWL58Ofr3749t27bh7rvvBsDPE1Cz88TPE3D+/Hns2rULXl5eWL16NbKzs/HCCy/g+vXrlnoIfp5qdp7c7fME1O5cVZSRkYHffvsN3333ndV+Z/tMuVwyYiYIgtXPoihW2gcARqMRI0eOxNtvv42WLVs65DldiaPPU48ePdCjRw/Lz71790bnzp3xxRdfYO7cuY4LvJ7Z82/fqlUrtGrVyvJzz549kZaWhk8++cTyJWvvc7oKR58nfp4Ak8kEQRDw7bffWq5uOnv2bDz66KOYN28evL297X5OV+Ho8+Sunyeg9v/+y5YtQ1BQEIYNG+aw56wLLjdMExISAqVSWSl7y8rKqpTlAUBeXh4OHjyIl156CSqVCiqVCjNnzsSRI0egUqmwZcsWAEB4eHiNn9MV1NV5uplCocCdd96JM2fO1Mn7qGv2nqeq9OjRw+ocePrnqSo3n6ebeeLnKSIiAo0bN7a6zHp8fDxEUcSlS5cA8PME1Ow83czVP0/A7f3fE0URS5YswZNPPgmNRmN1n7N9plwuGdFoNOjSpQs2b95stX/z5s3o1atXpeMDAgJw7NgxJCUlWW7jx49Hq1atkJSUhO7duwOQ/mq7+Tk3bdpk8zldQV2dp5uJooikpCRERETUyfuoa/aep6okJiZanQNP/zxV5ebzdDNP/Dz17t0b6enpyM/Pt+w7ffo0FAoFoqKiAPDzBNTsPN3M1T9PwO3939u+fTvOnj2LMWPGVLrP6T5T9V4y6wDmaU6LFy8WT548KU6cOFH09fW1VApPmTJFfPLJJ6t8vK1ZIrt37xaVSqX4wQcfiMnJyeIHH3zgNlPnHHmeZsyYIW7YsEE8d+6cmJiYKD7zzDOiSqUS//zzz7p8K3XK3vP02WefiatXrxZPnz4tHj9+XJwyZYoIQPz5558tx/DzVLPzxM+TKObl5YlRUVHio48+Kp44cULcvn272KJFC3Hs2LGWY/h5qtl5csfPkyjW/nf53//+d7F79+42n9PZPlMumYyIoijOmzdPjI2NFTUajdi5c2dx+/btlvueeuop8Z577qnysba+ZEVRFH/88UexVatWolqtFlu3bm31S9NVOfo8TZw4UYyJiRE1Go3YqFEjMSEhQdyzZ08dRV9/7DlPH374odisWTPRy8tLbNCggXjXXXeJ69atq/Scnv55qsl54udJkpycLA4YMED09vYWo6KixEmTJomFhYVWx3j650kUb32e3PXzJIr2n6ucnBzR29tb/Prrr6t8Tmf6TAmiKIry9MkQERERuWDNCBEREbkXJiNEREQkKyYjREREJCsmI0RERCQrJiNEREQkKyYjREREJCsmI0RERCQrJiNEREQkKyYjRFStCxcuQBAEJCUl1evrbtu2DYIgICcn57aeRxAErFmzpsr75Xp/RFSOyQiRBxMEodrb008/LXeIROQBVHIHQETyycjIsGyvXLkSb731Fk6dOmXZ5+3tjRs3btj9vEajEYIgQKHg3ztEdGv8TUHkwcLDwy23wMBACIJQaZ/Z+fPn0a9fP/j4+KBDhw7Yu3ev5b5ly5YhKCgIv/76K9q0aQOtVouLFy9Cr9fjjTfeQOPGjeHr64vu3btj27ZtlsddvHgRQ4cORYMGDeDr64u2bdti/fr1VjEeOnQIXbt2hY+PD3r16mWVLAHAggUL0KxZM2g0GrRq1QrLly+v9j3v378fnTp1gpeXF7p27YrExMTbOINE5AhMRoioRqZNm4bJkycjKSkJLVu2xBNPPAGDwWC5v7CwELNmzcKiRYtw4sQJhIaG4plnnsHu3bvx/fff4+jRo3jssccwaNAgnDlzBgDw4osvoqSkBDt27MCxY8fw4Ycfws/Pr9Lrfvrppzh48CBUKhWeffZZy32rV6/GK6+8gtdeew3Hjx/HP/7xDzzzzDPYunWrzfdQUFCABx54AK1atcKhQ4cwY8YMTJ48uQ7OFhHZRbbrBRORU1m6dKkYGBhYaX9KSooIQFy0aJFl34kTJ0QAYnJysuWxAMSkpCTLMWfPnhUFQRAvX75s9Xz9+/cXp06dKoqiKN5xxx3ijBkzbMazdetWEYD4+++/W/atW7dOBCAWFRWJoiiKvXr1EseNG2f1uMcee0y8//77LT8DEFevXi2KoiguXLhQDA4OFgsKCiz3L1iwQAQgJiYmVnVqiKiOsWeEiGqkffv2lu2IiAgAQFZWlmWfRqOxOubw4cMQRREtW7aEn5+f5bZ9+3acO3cOADBhwgS8++676N27N6ZPn46jR4/a9brJycno3bu31fG9e/dGcnKyzfeQnJyMDh06wMfHx7KvZ8+eNTsBRFRnWMBKRDWiVqst24IgAABMJpNln7e3t2W/+T6lUolDhw5BqVRaPZd5KGbs2LEYOHAg1q1bh02bNmHWrFn49NNP8fLLL9f4dSu+JgCIolhpX8X7iMj5sGeEiOpEp06dYDQakZWVhebNm1vdwsPDLcdFR0dj/PjxWLVqFV577TV88803NX6N+Ph47Nq1y2rfnj17EB8fb/P4Nm3a4MiRIygqKrLs27dvn53vjIgcjckIEdWJli1bYtSoURg9ejRWrVqFlJQUHDhwAB9++KFlxszEiROxceNGpKSk4PDhw9iyZUuViYQtr7/+OpYtW4avvvoKZ86cwezZs7Fq1aoqi1JHjhwJhUKBMWPG4OTJk1i/fj0++eQTh7xfIqo9JiNEVGeWLl2K0aNH47XXXkOrVq3w4IMP4s8//0R0dDQAaT2SF198EfHx8Rg0aBBatWqF+fPn1/j5hw0bhs8//xwff/wx2rZti4ULF2Lp0qXo27evzeP9/Pzwyy+/4OTJk+jUqROmTZuGDz/80BFvlYhugyByEJWIiIhkxJ4RIiIikhWTESIiIpIVkxEiIiKSFZMRIiIikhWTESIiIpIVkxEiIiKSFZMRIiIikhWTESIiIpIVkxEiIiKSFZMRIiIikhWTESIiIpLV/wPfyhnKgZkqcAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], label='Precision')\n",
    "plt.plot(thresholds, recalls[:-1], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "40d71f47-ff9e-414e-a393-d5c973d9212c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4561, number of negative: 4047\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2303\n",
      "[LightGBM] [Info] Number of data points in the train set: 8608, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.74      0.57       988\n",
      "           1       0.55      0.27      0.36      1167\n",
      "\n",
      "    accuracy                           0.48      2155\n",
      "   macro avg       0.50      0.50      0.46      2155\n",
      "weighted avg       0.51      0.48      0.45      2155\n",
      "\n",
      "Train: 0.6009526022304833\n",
      "Test: 0.4835266821345708\n",
      "[LightGBM] [Info] Number of positive: 951, number of negative: 847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 1798, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 1878, number of negative: 1713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 3591, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 2819, number of negative: 2565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 5384, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 3789, number of negative: 3388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2301\n",
      "[LightGBM] [Info] Number of data points in the train set: 7177, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 4760, number of negative: 4210\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2303\n",
      "[LightGBM] [Info] Number of data points in the train set: 8970, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Scores: [0.46681539 0.49804796 0.53039598 0.48800892 0.51143335]\n",
      "Mean: 0.49894032348020084\n",
      "[LightGBM] [Info] Number of positive: 951, number of negative: 847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 1798, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 1878, number of negative: 1713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 3591, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 2819, number of negative: 2565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 5384, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 3789, number of negative: 3388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2301\n",
      "[LightGBM] [Info] Number of data points in the train set: 7177, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 4760, number of negative: 4210\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2303\n",
      "[LightGBM] [Info] Number of data points in the train set: 8970, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Mean CV F1: 0.483\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    num_leaves=15,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=-8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha = 5.0,\n",
    "    reg_lambda = 3.0,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "data_split(X,y,lgb)\n",
    "\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(lgb, X, y, cv=tscv, scoring='f1')\n",
    "print(f\"Mean CV F1: {scores.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c59a3081-5987-45dd-a1ba-2fc5f12a045f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4561, number of negative: 4047\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2303\n",
      "[LightGBM] [Info] Number of data points in the train set: 8608, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.68      0.55       988\n",
      "           1       0.54      0.31      0.40      1167\n",
      "\n",
      "    accuracy                           0.48      2155\n",
      "   macro avg       0.50      0.50      0.47      2155\n",
      "weighted avg       0.50      0.48      0.47      2155\n",
      "\n",
      "Train: 0.6463754646840149\n",
      "Test: 0.48259860788863107\n",
      "[LightGBM] [Info] Number of positive: 951, number of negative: 847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 1798, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1878, number of negative: 1713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 3591, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 2819, number of negative: 2565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 5384, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 3789, number of negative: 3388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2301\n",
      "[LightGBM] [Info] Number of data points in the train set: 7177, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 4760, number of negative: 4210\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2303\n",
      "[LightGBM] [Info] Number of data points in the train set: 8970, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Scores: [0.48466258 0.49916341 0.53151143 0.46904629 0.5103179 ]\n",
      "Mean: 0.49894032348020084\n",
      "[LightGBM] [Info] Number of positive: 951, number of negative: 847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 1798, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1878, number of negative: 1713\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 3591, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 2819, number of negative: 2565\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2299\n",
      "[LightGBM] [Info] Number of data points in the train set: 5384, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 3789, number of negative: 3388\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2301\n",
      "[LightGBM] [Info] Number of data points in the train set: 7177, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 4760, number of negative: 4210\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2303\n",
      "[LightGBM] [Info] Number of data points in the train set: 8970, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Mean CV F1: 0.486\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=-8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha = 5.0,\n",
    "    reg_lambda = 3.0,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "data_split(X_std,y,lgb)\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(lgb, X, y, cv=tscv, scoring='f1')\n",
    "print(f\"Mean CV F1: {scores.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "65a5d639-91d7-411f-8603-50050d08656a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.02       988\n",
      "           1       0.54      1.00      0.70      1167\n",
      "\n",
      "    accuracy                           0.54      2155\n",
      "   macro avg       0.60      0.50      0.36      2155\n",
      "weighted avg       0.60      0.54      0.39      2155\n",
      "\n",
      "Train: 0.5284618959107806\n",
      "Test: 0.5438515081206496\n",
      "Scores: [0.47796988 0.47629671 0.52258784 0.46681539 0.54322365]\n",
      "Mean: 0.49737869492470715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           Close     Volume           RAV  Volitility  Buy_Sell_Strength  \\\n",
       " 0      46.935463  692573000  5.937243e+08    1.645283           0.965530   \n",
       " 1     231.279999  135702700  1.130020e+08   15.801932           0.926406   \n",
       " 2     317.101318   24040000  2.155843e+07    8.276232           0.844679   \n",
       " 3     233.190002  130442800  1.174883e+08   15.491407           0.322981   \n",
       " 4      45.637337  755293000  6.254683e+08    1.644891           0.117379   \n",
       " ...          ...        ...           ...         ...                ...   \n",
       " 2150  510.959991   24065400  1.830163e+07    6.116654           0.095339   \n",
       " 2151  249.100006   17891929  4.003839e+07    6.819915           0.900764   \n",
       " 2152  513.518494    6387641  1.618231e+07    6.132103           0.388691   \n",
       " 2153  188.149994   89180834  1.577492e+08    5.938565           0.526582   \n",
       " 2154  426.689209   38039420  8.715610e+07   11.718095           0.752337   \n",
       " \n",
       "       Weighted_Strength     Trend   Returns  Log_returns   AAPL   MSFT   NVDA  \\\n",
       " 0              0.543035 -0.048846  0.084713    12.458125  False  False   True   \n",
       " 1              0.512066 -0.056184  0.073275    43.045538  False  False  False   \n",
       " 2              0.384355 -0.057114  0.017063    55.221980  False   True  False   \n",
       " 3             -0.196537 -0.074962  0.008258    42.837232  False  False  False   \n",
       " 4             -0.462040 -0.076714 -0.027658    11.857631  False  False   True   \n",
       " ...                 ...       ...       ...          ...    ...    ...    ...   \n",
       " 2150          -0.532101  0.038578 -0.021899    81.643431  False   True  False   \n",
       " 2151           0.179089  0.033857  0.015615    45.271487   True  False  False   \n",
       " 2152          -0.043937  0.033574  0.005007    82.343571  False   True  False   \n",
       " 2153           0.015028  0.026759  0.027244    36.110747  False  False   True   \n",
       " 2154           0.110133  0.061585  0.031921    70.824096  False  False  False   \n",
       " \n",
       "        TSLA  \n",
       " 0     False  \n",
       " 1      True  \n",
       " 2     False  \n",
       " 3      True  \n",
       " 4     False  \n",
       " ...     ...  \n",
       " 2150  False  \n",
       " 2151  False  \n",
       " 2152  False  \n",
       " 2153  False  \n",
       " 2154   True  \n",
       " \n",
       " [2155 rows x 13 columns],\n",
       " 0       0\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2150    1\n",
       " 2151    0\n",
       " 2152    0\n",
       " 2153    0\n",
       " 2154    0\n",
       " Name: Target, Length: 2155, dtype: int64)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression(fit_intercept=False, class_weight=\"balanced\", C=0.2)\n",
    "\n",
    "data_split(X,y,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ef241ff8-cec0-4c9f-9f63-6e1725092e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.500, Best F1: nan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63       825\n",
      "           1       0.00      0.00      0.00       968\n",
      "\n",
      "    accuracy                           0.46      1793\n",
      "   macro avg       0.23      0.50      0.31      1793\n",
      "weighted avg       0.21      0.46      0.29      1793\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalu\\AppData\\Local\\Temp\\ipykernel_22036\\584236959.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Get prediction probabilities from your model\n",
    "y_probs = log.predict_proba(log_X_test)[:, 1]  # probabilities for class 1\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(log_y_test, y_probs)\n",
    "\n",
    "# Compute F1 for each threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "\n",
    "# Find best threshold for highest F1\n",
    "best_idx = f1_scores.argmax()\n",
    "best_thresh = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"Best Threshold: {best_thresh:.3f}, Best F1: {best_f1:.3f}\")\n",
    "\n",
    "# Evaluate using that threshold\n",
    "y_pred_opt = (y_probs >= best_thresh).astype(int)\n",
    "print(classification_report(log_y_test, y_pred_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "35cb0aed-5bee-415e-b4b8-055a41a8d163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.02       988\n",
      "           1       0.54      1.00      0.70      1167\n",
      "\n",
      "    accuracy                           0.54      2155\n",
      "   macro avg       0.60      0.50      0.36      2155\n",
      "weighted avg       0.60      0.54      0.39      2155\n",
      "\n",
      "Train: 0.5284618959107806\n",
      "Test: 0.5438515081206496\n",
      "Scores: [0.47796988 0.47629671 0.52258784 0.46681539 0.54322365]\n",
      "Mean: 0.49737869492470715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           Close     Volume           RAV  Volitility  Buy_Sell_Strength  \\\n",
       " 0      46.935463  692573000  5.937243e+08    1.645283           0.965530   \n",
       " 1     231.279999  135702700  1.130020e+08   15.801932           0.926406   \n",
       " 2     317.101318   24040000  2.155843e+07    8.276232           0.844679   \n",
       " 3     233.190002  130442800  1.174883e+08   15.491407           0.322981   \n",
       " 4      45.637337  755293000  6.254683e+08    1.644891           0.117379   \n",
       " ...          ...        ...           ...         ...                ...   \n",
       " 2150  510.959991   24065400  1.830163e+07    6.116654           0.095339   \n",
       " 2151  249.100006   17891929  4.003839e+07    6.819915           0.900764   \n",
       " 2152  513.518494    6387641  1.618231e+07    6.132103           0.388691   \n",
       " 2153  188.149994   89180834  1.577492e+08    5.938565           0.526582   \n",
       " 2154  426.689209   38039420  8.715610e+07   11.718095           0.752337   \n",
       " \n",
       "       Weighted_Strength     Trend   Returns  Log_returns   AAPL   MSFT   NVDA  \\\n",
       " 0              0.543035 -0.048846  0.084713    12.458125  False  False   True   \n",
       " 1              0.512066 -0.056184  0.073275    43.045538  False  False  False   \n",
       " 2              0.384355 -0.057114  0.017063    55.221980  False   True  False   \n",
       " 3             -0.196537 -0.074962  0.008258    42.837232  False  False  False   \n",
       " 4             -0.462040 -0.076714 -0.027658    11.857631  False  False   True   \n",
       " ...                 ...       ...       ...          ...    ...    ...    ...   \n",
       " 2150          -0.532101  0.038578 -0.021899    81.643431  False   True  False   \n",
       " 2151           0.179089  0.033857  0.015615    45.271487   True  False  False   \n",
       " 2152          -0.043937  0.033574  0.005007    82.343571  False   True  False   \n",
       " 2153           0.015028  0.026759  0.027244    36.110747  False  False   True   \n",
       " 2154           0.110133  0.061585  0.031921    70.824096  False  False  False   \n",
       " \n",
       "        TSLA  \n",
       " 0     False  \n",
       " 1      True  \n",
       " 2     False  \n",
       " 3      True  \n",
       " 4     False  \n",
       " ...     ...  \n",
       " 2150  False  \n",
       " 2151  False  \n",
       " 2152  False  \n",
       " 2153  False  \n",
       " 2154   True  \n",
       " \n",
       " [2155 rows x 13 columns],\n",
       " 0       0\n",
       " 1       1\n",
       " 2       1\n",
       " 3       1\n",
       " 4       1\n",
       "        ..\n",
       " 2150    1\n",
       " 2151    0\n",
       " 2152    0\n",
       " 2153    0\n",
       " 2154    0\n",
       " Name: Target, Length: 2155, dtype: int64)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression(fit_intercept=False, class_weight=\"balanced\", C=0.2)\n",
    "\n",
    "data_split(X_std,y,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "eee4c088-7f7c-4367-8d11-707a8f06698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=42)\n",
    "# lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "# xgb = XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# v = VotingClassifier(estimators=[('rf', rf), ('lr', lr), ('xgb', xgb)], voting='soft')\n",
    "\n",
    "\n",
    "\n",
    "# for train_idx, test_idx in tscv.split(X_std):\n",
    "#     v_X_train, v_X_test = X_std.iloc[train_idx], X_std.iloc[test_idx]\n",
    "#     v_y_train, v_y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "#     v.fit(v_X_train,v_y_train)\n",
    "#     v_pred = v.predict(v_X_test)\n",
    "#     v_probs = v.predict_proba(X_test)[:,1]\n",
    "#     y_pred = (probs >= 0.5).astype(int) \n",
    "\n",
    "# print(classification_report(v_y_test,v_pred))\n",
    "# print(f'Train: {v.score(v_X_train,v_y_train)}')\n",
    "# print(f'Test: {v.score(v_X_test,v_y_test)}')\n",
    "\n",
    "# scores = cross_val_score(v, X, y, cv=tscv, scoring='accuracy')\n",
    "# print(\"Scores:\", scores)\n",
    "# print(\"Mean:\", scores.mean())\n",
    " # adjust threshold if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e94863ff-382e-4445-bb08-c260c4badd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5035, 5728], dtype=int64)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b021c709-0fda-421d-8120-3249ad239595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost normal [ 234 1559]\n",
      "xgboost std [1717   76]\n",
      "rf normal [850 943]\n",
      "rf std [841 952]\n"
     ]
    }
   ],
   "source": [
    "print(\"xgboost normal\",np.bincount(preds))\n",
    "print(\"xgboost std\",np.bincount(std_preds))\n",
    "print(\"rf normal\",np.bincount(rf_pred))\n",
    "print(\"rf std\",np.bincount(rf_std_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e2d84739-7409-425e-9da8-d656cdab8640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_before</th>\n",
       "      <th>importance_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Returns</td>\n",
       "      <td>0.187211</td>\n",
       "      <td>0.181571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAV</td>\n",
       "      <td>0.147089</td>\n",
       "      <td>0.161016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buy_Sell_Strength</td>\n",
       "      <td>0.127989</td>\n",
       "      <td>0.145201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Weighted_Strength</td>\n",
       "      <td>0.122289</td>\n",
       "      <td>0.129612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Close</td>\n",
       "      <td>0.103094</td>\n",
       "      <td>0.096351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volume</td>\n",
       "      <td>0.096170</td>\n",
       "      <td>0.075545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Volitility</td>\n",
       "      <td>0.092091</td>\n",
       "      <td>0.097002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Log_returns</td>\n",
       "      <td>0.068565</td>\n",
       "      <td>0.063649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trend</td>\n",
       "      <td>0.048425</td>\n",
       "      <td>0.040034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>0.002891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.001775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.004954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance_before  importance_after\n",
       "7             Returns           0.187211          0.181571\n",
       "2                 RAV           0.147089          0.161016\n",
       "4   Buy_Sell_Strength           0.127989          0.145201\n",
       "5   Weighted_Strength           0.122289          0.129612\n",
       "0               Close           0.103094          0.096351\n",
       "1              Volume           0.096170          0.075545\n",
       "3          Volitility           0.092091          0.097002\n",
       "8         Log_returns           0.068565          0.063649\n",
       "6               Trend           0.048425          0.040034\n",
       "12               TSLA           0.003136          0.002891\n",
       "11               NVDA           0.002238          0.001775\n",
       "10               MSFT           0.001450          0.004954\n",
       "9                AAPL           0.000251          0.000400"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance_before': rf_classifier.feature_importances_,\n",
    "    'importance_after': rf_std_classifier.feature_importances_,\n",
    "}).sort_values('importance_before', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1b97a763-3775-4690-bc2e-c75962f22410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RAV</th>\n",
       "      <th>Volitility</th>\n",
       "      <th>Buy_Sell_Strength</th>\n",
       "      <th>Weighted_Strength</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Log_returns</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.935463</td>\n",
       "      <td>692573000</td>\n",
       "      <td>5.937243e+08</td>\n",
       "      <td>1.645283</td>\n",
       "      <td>0.965530</td>\n",
       "      <td>0.543035</td>\n",
       "      <td>-0.048846</td>\n",
       "      <td>0.084713</td>\n",
       "      <td>12.458125</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231.279999</td>\n",
       "      <td>135702700</td>\n",
       "      <td>1.130020e+08</td>\n",
       "      <td>15.801932</td>\n",
       "      <td>0.926406</td>\n",
       "      <td>0.512066</td>\n",
       "      <td>-0.056184</td>\n",
       "      <td>0.073275</td>\n",
       "      <td>43.045538</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>317.101318</td>\n",
       "      <td>24040000</td>\n",
       "      <td>2.155843e+07</td>\n",
       "      <td>8.276232</td>\n",
       "      <td>0.844679</td>\n",
       "      <td>0.384355</td>\n",
       "      <td>-0.057114</td>\n",
       "      <td>0.017063</td>\n",
       "      <td>55.221980</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233.190002</td>\n",
       "      <td>130442800</td>\n",
       "      <td>1.174883e+08</td>\n",
       "      <td>15.491407</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>-0.196537</td>\n",
       "      <td>-0.074962</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>42.837232</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.637337</td>\n",
       "      <td>755293000</td>\n",
       "      <td>6.254683e+08</td>\n",
       "      <td>1.644891</td>\n",
       "      <td>0.117379</td>\n",
       "      <td>-0.462040</td>\n",
       "      <td>-0.076714</td>\n",
       "      <td>-0.027658</td>\n",
       "      <td>11.857631</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close     Volume           RAV  Volitility  Buy_Sell_Strength  \\\n",
       "0   46.935463  692573000  5.937243e+08    1.645283           0.965530   \n",
       "1  231.279999  135702700  1.130020e+08   15.801932           0.926406   \n",
       "2  317.101318   24040000  2.155843e+07    8.276232           0.844679   \n",
       "3  233.190002  130442800  1.174883e+08   15.491407           0.322981   \n",
       "4   45.637337  755293000  6.254683e+08    1.644891           0.117379   \n",
       "\n",
       "   Weighted_Strength     Trend   Returns  Log_returns   AAPL   MSFT   NVDA  \\\n",
       "0           0.543035 -0.048846  0.084713    12.458125  False  False   True   \n",
       "1           0.512066 -0.056184  0.073275    43.045538  False  False  False   \n",
       "2           0.384355 -0.057114  0.017063    55.221980  False   True  False   \n",
       "3          -0.196537 -0.074962  0.008258    42.837232  False  False  False   \n",
       "4          -0.462040 -0.076714 -0.027658    11.857631  False  False   True   \n",
       "\n",
       "    TSLA  \n",
       "0  False  \n",
       "1   True  \n",
       "2  False  \n",
       "3   True  \n",
       "4  False  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230fe7a-a886-419f-b744-602e3c8681d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
